Appendix--Overview (Including Data Collection Instruments) Table A-O-1 MSAP Grants Awarded in 1985 through 1998, By State, City, and District Year of MSAP Award 1989 1991 1993 1995 Total no. of awards 1 X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X 1 2 1 4 1 2 2 3 2 1 1 1 1 1 1 6 2 1 1 1
State and City Alabama Birmingham Gadsden Huntsville Mobile Montgomery Tuscaloosa Arizona Glendale Phoenix Phoenix Tucson Arkansas Forrest City Little Rock Pine Bluff California Berkeley Cerritos Fresno Long Beach Los Angeles Moreno Valley National City Redwood City
District Birmingham City Public Schools Gadsden County School District Huntsville City Mobile County Public Schools Montgomery Public Schools Tuscaloosa City School District Maricopa County District #40 Phoenix Elementary School District #1 Phoenix Union High School District Tucson Unified School District Forrest City Public Schools Little Rock School District Pine Bluff School District #3 Berkeley Unified School District ABC Unified School District Fresno Unified School District Long Beach Unified School District Los Angeles Unified School District Moreno Valley Unified School District National Elementary School District Redwood City Elem School District
1985 X
1987
1998
Table A-O-1 (continued) MSAP Grants Awarded in 1985 through 1998, By State, City, and District Year of MSAP Award 1989 1991 1993 1995 X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X Total no. of awards 1 4 5 2 3 4 6 2 2 1 3 1 4 1 1 1 3 3 4 2 3
State and City District 1985 California (continued) Richmond Richmond Unified School District Sacramento Sacramento City Unified Sch District San Diego San Diego Unified X School District San Francisco San Francisco Unified Sch District San Jose East Side Union High School District San Jose San Jose Unified School District Stockton Stockton Unified X School District Colorado Denver Denver Public Schools X Connecticut Bridgeport Bridgeport Board of Education Capitol Region Education Council East Lyme Project LEARN New Britain New Haven Stamford District of Columbia Florida Bartow Fort Lauderdale Fort Myers Fort Pierce Jacksonville Largo New Britain School District New Haven Public Schools Stamford School District District of Columbia Public Schools Polk County School Board Broward County School Board Lee County School District St. Lucie County School Board Duval County School Board Pinellas County School Board
1987
1998
Table A-O-1 (continued) MSAP Grants Awarded in 1985 through 1998, By State, City, and District Year of MSAP Award 1989 1991 1993 1995 X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X Total no. of awards 1 3 2 1 1 2 3 3 1 2 1 3 3 2 1 2 6 2 1 4
State and City District Florida (continued) Melbourne Brevard County School Board Miami Dade County School Board Pensacola Escambia County School District Quincy Gadsden County School District Sanford Seminole County School District Tampa Hillsborough County West Palm Beach Georgia Macon Columbus Savannah Waycross Illinois Chicago Decatur Kankakee Rockford Indiana Fort Wayne Indianapolis Kansas Topeka Kentucky Lexington Louisville Palm Beach County School Board Bibb County School District Muscogee County Savannah/Chatham Public Schools Ware County School District Chicago Public Schools Decatur Public School District #361 Kankakee School District #111 Rockford School District #205
1985
1987
1998
Ft. Wayne Community Schools Indianapolis Public X Schools Topeka Public Schools Fayette County Public Schools Jefferson County Board of Education
Table A-O-1 (continued) MSAP Grants Awarded in 1985 through 1998, By State, City, and District Year of MSAP Award 1989 1991 1993 1995 Total no. of awards 1 1 1 X X X X X X X X X X X X X X X X X X X X X X X X X 2 1 1 1 1 1 3 2 1 2 2 1 1 3 1 2
State and City Louisiana Monroe New Orleans New Roads Reserve Ruston Shreveport Maryland Rockville
District City of Monroe Parish School Board Orleans Parish School Board Pointe Coupee Parish School Board Saint John Parish School Board Grambling State University Caddo Parish School Board
1985
1987
1998 X
X X
Montgomery County Public Schools Towson Baltimore County Public Schools Upper Marlboro Prince Georges County Public Schools Massachusetts Boston Boston Public Schools Lawrence Lowell New Bedford Springfield Michigan Benton Harbor Flint Grand Rapids Kalamazoo River Rouge Lawrence Public Schools Lowell Public Schools New Bedford Public Schools Springfield School District Benton Harbor Area Schools Flint City School District Grand Rapids Public Schools Kalamazoo Public Schools School District/City of River Rouge
Table A-O-1 (continued) MSAP Grants Awarded in 1985 through 1998, By State, City, and District Year of MSAP Award 1989 1991 1993 1995 X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X Total no. of awards 4 4 1 2 1 2 1 1 2 3 2 2 3 3 1 1 2 4 2 1
State and City Minnesota Minneapolis St. Paul Mississippi Canton Cleveland Hattiesburg Jackson Laurel Vicksburg Missouri Kansas City St. Louis Nebraska Omaha Nevada Las Vegas New Jersey Bayonne Montclair Teaneck New York Albany Beacon Buffalo Freeport Mount Vernon
District Minneapolis Public Schools St. Paul Public Schools ISD #625 Madison County School District Cleveland (MS) School District Hattiesburg Public School District Jackson County School District Laurel Vicksburg Warren School District Kansas City MO School District St. Louis City School District Omaha Public School District #1 Clark County School District Bayonne City School District Montclair Public Schools Teaneck Township Albany Beacon City School District Buffalo City School District Freeport U F School District Mount Vernon Public Schools
1985 X X
1987
1998
Table A-O-1 (continued) MSAP Grants Awarded in 1985 through 1998, By State, City, and District Year of MSAP Award 1989 1991 1993 1995 X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X Total no. of awards 2 2 5 5 2 2 3 4 1 4 3 5 4 2 2 3 2 1 4 1 4 6 2
State and City NYC Area Brooklyn NY 10002 NY 10001 NY 10025 NY 10035 Bronx 10458 Brooklyn 11231 Brooklyn 11236 Brooklyn 11207 Brooklyn 11219 Brooklyn 11224 Brooklyn 11235 Flushing 11365 Bayside 11364 Forest Hills 11375 Jackson Hts 11370 Brooklyn 11201 New York state New Rochelle Newburgh Port Chester Poughkeepsie Rochester Schnectady
District NYC Bd of Ed/ Div of High School NYC Community School District #1 NYC Community School District #2 NYC Community School District #3 NYC Community School District #4 NYC Community School District #10 NYC Community School District #15 NYC Community School District #18 NYC Community School District #19 NYC Community School District #20 NYC Community School District #21 NYC Community School District #22 NYC Community School District #25 NYC Community School District #26 NYC Community School District #28 NYC Community School District #30 NYC Community School District #33 New Rochelle Newburgh City School District Pt Chester-Rye Union Free School District Poughkeepsie City School District Rochester City School District Schnectady City School District
1985
1987
1998
Table A-O-1 (continued) MSAP Grants Awarded in 1985 through 1998, By State, City, and District Year of MSAP Award 1989 1991 1993 1995 X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X Total no. of awards 2 3 4 1 3 1 1 2 4 2 1 1 2 1 4 2 4 1 3 1
State and City District 1985 New York (continued) Utica Utica City School District White Plains White Plains School District Yonkers Yonkers City School District North Carolina Asheville Asheville City Schools Charlotte Durham Elizabethtown Greensboro Raleigh Tarboro Ohio Akron Canton Cincinnati Cincinnati Cleveland Columbus Dayton Lima Lorain University Heights Charlotte-Mecklenburg Durham County Schools Bladen County Schools Greensboro Public Schools Wake County Public School System Edgecomb County Schools Akron Plain Local School District Cincinnati Board of Education Greenhills-Forest Park City Cleveland City School District Columbus City School District Dayton Public Schools Lima Lorain Board of Education Cleveland HeightsUniversity Hts City
1987
1998
Table A-O-1 (continued) MSAP Grants Awarded in 1985 through 1998, By State, City, and District Year of MSAP Award 1989 1991 1993 1995 Total no. of awards 1 2 3 1 X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X 5 1 1 1 2 3 1 2 4 1 1 2 2 3 3
State and City Oklahoma Oklahoma City Oregon Portland Pennsylvania Philadelphia Pittsburgh Rhode Island Providence South Carolina Darlington Tennessee Chattanooga Jackson Nashville Texas Amarillo Austin Corpus Christi Dallas Fort Worth Houston Houston Odessa Victoria Wichita Falls
District Oklahoma City Public School District #1-89 Portland School District #1 Philadelphia School District Pittsburgh School District Providence Public School Dept Darlington County Schools Hamilton County School District Jackson-Madison County Sch District Nashville-Davidson County Sch District Amarillo Independent School District Austin Independent School District Corpus Christi Indep School District Dallas Independent School District Fort Worth Indep School District Aldine Independent School District Houston Independent School District Ector County Indep School District Victoria Independent School District Wichita Falls Indep School District
1985
1987
1998 X
X X X X X X
X
Table A-O-1 (continued) MSAP Grants Awarded in 1985 through 1998, By State, City, and District Year of MSAP Award 1989 1991 1993 1995 Total no. of awards 1 X X X X X X X X X X X X 44 38 X 54 64 58 64 57 X X X X X X X X 1 1 6 5 4 2 2 379
State and City Virginia Alexandra Danville Lynchburg Roanoke Washington Seattle Tacoma Yakima Wisconsin Milwaukee Totals Total no. states Average no. grants per district
District Alexandra Public Schools Danville City Public Schools Lynchburg Roanoke City Public Schools Seattle Public Schools Tacoma School District #10 Yakima School District Milwaukee Public Schools 171 35 and District of Columbia 2.2
1985
1987 X
1998
Table A-O-1 (continued) Number of Grants Received Per District: 1985 through 1998
No. of Grants Received One Two Three Four Five Six Total number of districts
No. of Districts 67 47 26 20 6 5 171
% of Total Grants Awarded 39.2% 27.5 15.2 11.7 3.5 2.9 100.0
Table A-O-2 Evaluation Questions for the National Evaluation of the Magnet Schools Assistance Program (MSAP) and Where They Are Addressed in Study Reports
Evaluation Question I. What are the characteristics of MSAP projects? A.
The types of MSAP-supported schools 1.
What proportion of MSAP-targeted schools are whole school vs. program within a school (PWS)? 2.
How many MSAP projects serve each grade level? How many elementary and secondary students are enrolled in MSAP projects? 3.
How many schools in MSAP-funded districts are targeted for desegregation impact? How many schools are designated as feeder schools? 4.
What is the demographic composition of MSAP project target and feeder school students? 5.
How do MSAP teachers and principals compare to those in other public schools in terms of background and demographic characteristics? B.
The school choice process in MSAP-supported districts 1.
To what extent do MSAP programs select students on the basis of prior achievement or expected performance? 2.
What is the frequency and extent of waiting lists? 3.
What kinds of school choice other than those supported by MSAP are available to students and families in MSAP districts? C.
The accountability and funding of MSAP-supported schools 1.
Are MSAP-supported schools more autonomous and/or accountable to local education agencies (LEAs) than other public schools, and if so, in what ways? 2.
In addition to MSAP grants, what other sources of support are available to and accessed by MSAP projects and schools? 3.
Do magnet schools and districts receive and coordinate other federal funding for which they are eligible?
Where Addressed Chapter I Chapter I Targeted schools: Chapter III; feeders: Year 2 Evaluation Report Chapter I Year 2 Evaluation Report
Chapter III Chapter II Chapter II
Chapter IV Chapter VII Chapter IV
Table A-O-2 (continued) Evaluation Questions for the National Evaluation of the Magnet Schools Assistance Program (MSAP) and Where They Are Addressed in Study Reports
Evaluation Question II.
What are the characteristics of MSAP districts? A.
The context of MSAP districts 1.
What are the demographic compositions, sizes, and urbanicities of MSAP districts? 2.
What are the enrollment trends of schools in MSAP districts by racial/ethnic composition? 3.
How many MSAP districts are operating under a court order vs. implementing a voluntary desegregation plan? 4.
What are the desegregation trends in the school systems in which MSAP projects operate? 5.
Are there differences in desegregation trends for different minority groups? III.
To what extent are federally funded magnet projects reducing the incidence or degree of minority student isolation in their programs? A. Desegregation goals and outcomes in MSAP magnet schools and targeted feeder schools 1.
What are the desegregation objectives of the MSAP targeted schools? How many have annual benchmarks and what are they? 2.
What recruitment strategies do MSAP projects implement to meet their desegregation goals? 3.
What progress do MSAP targeted schools make in meeting their desegregation objectives? 4.
How do district enrollment trends and/or other factors influence MSAP targeted schools' ability to meet their desegregation objectives? 5.
Do magnet school courses and program activities within a school reflect a similar minority/non-minority distribution as the school as a whole (or the PWS)? 6.
Are there differences in desegregation outcomes within a school for different minority groups?
Where Addressed Chapter II Chapter II Chapter II Chapter II Year 2 and 3 Evaluation Reports
Chapter III Chapter III Year 2 and 3 Evaluation Reports Year 2 and 3 Evaluation Reports Year 2 and 3 Evaluation Reports Year 2 and 3 Evaluation Reports
Table A-O-2 (continued) Evaluation Questions for the National Evaluation of the Magnet Schools Assistance Program (MSAP) and Where They Are Addressed in Study Reports
Evaluation Question IV.
To what extent are federally funded magnet projects promoting systemic, standards-based reform? A. Role of MSAP projects in supporting systemic reform efforts 1.
To what extent are magnet projects and schools involved in supporting national, state, or local systemic reform efforts? 2.
To what extent do magnet schools engage in efforts to align curricula and instruction with challenging state or district standards? 3.
Are magnet projects using/consolidating Title I, Goals 2000, and other federal funds to promote these reforms? 4.
To what extent, if any, is there a tension between what is required by state or district standards and the magnet schools' missions, philosophies, or curricula? V.
To what extent do federally funded magnet projects feature innovative educational methods and practices that meet identified student needs and interests? A. Curricula and instruction in MSAP schools 1.
What kinds of educational methods, practices, and curricula do MSAP schools employ? 2.
Are they research-based? 3.
How are student needs and interests gauged, and how are they incorporated into magnet projects? 4.
To what extent and in what ways are the needs of students of different racial, ethnic, and socioeconomic backgrounds explicitly taken into account? 5.
Does the curriculum provide a coherent, challenging program for all students? 6.
What activities are counted by magnet projects as "innovative"?
Where Addressed
Chapter IV Chapter IV Chapter IV Chapter IV
Chapter V Chapter V Chapter V Chapter V Chapter V Year 2 Evaluation Report
Table A-O-2 (continued) Evaluation Questions for the National Evaluation of the Magnet Schools Assistance Program (MSAP) and Where They Are Addressed in Study Reports
Evaluation Question B.
The role of MSAP schools as models for other schools 1.
To what extent and in what ways do the innovative practices implemented in MSAP schools serve as models for school improvement? Which practices? 2.
Do MSAP projects, local education agencies (LEAs), and state education agencies (SEAs) formally or informally share information about promising innovative practices for public education generally? What can the successes and failures of magnet projects tell policy makers and practitioners about innovative practices? VI.
To what extent do federally funded magnet projects strengthen students' knowledge of academic subjects and skills needed for successful careers in the future? A.
The setting of achievement objectives and measuring progress 1.
What types of achievement objectives do MSAP projects set for their schools? 2.
What kinds of performance data (e.g., standardized assessments, portfolios, course-taking, attendance) do magnet projects collect, and how often? 3.
What kinds of baseline or comparison groups do projects utilize in assessing achievement outcomes? 4.
What kinds of analyses do projects conduct and report? B. Achievement outcomes 1.
Do magnet schools meet or exceed the achievement benchmarks/goals set forth in the project's applications? 2.
Do magnet schools show achievement gains or improved achievement trends over time in core subjects and special skill areas (where applicable)? 3.
Are achievement gains of students in magnet programs statistically and substantively different from those of comparable students in other public schools in the district, state, or nation? 4.
Do magnet programs foster gains in the achievement of both minority and non-minority students and of both high and low poverty students? 5.
What characteristics of magnet schools account for magnet student achievement gains or losses? 6.
Is there evidence that districts with well-functioning magnet projects show achievement gains in all schools?
Where Addressed Chapter V Year 2 and 3 Evaluation Reports
Chapter VI Chapter VI Chapter VI Plans: Chapter VI; description of actual: Year 2 and 3 Evaluation Year 2 and 3 Evaluation Reports Year 2 and 3 Evaluation Reports Year 2 and 3 Evaluation Reports Year 2 and 3 Evaluation Reports Year 2 and 3 Evaluation Reports Year 2 and 3 Evaluation Reports
Table A-O-2 (continued) Evaluation Questions for the National Evaluation of the Magnet Schools Assistance Program (MSAP) and Where They Are Addressed in Study Reports
Evaluation Question VII.
How has the MSAP contributed to the development and implementation of magnet projects? A. MSAP project planning and implementation processes 1.
What kinds of planning and implementation activities do MSAP grants support? 2.
How do MSAP projects spend their MSAP funds? 3.
What did federal funds allow magnet projects to do that they otherwise could not have done? 4.
What proportion of annual costs of educating students in MSAP schools (less transportation) does the MSAP grant cover? 5.
Does the distribution of funds differ by year of implementation, and if so, how? 6.
Do MSAP grantees plan to continue their programs after their grant expires? Do they continue? B. Federal MSAP program processes and guidance 1.
How does MSAP award grants? 2.
What accounts for differences in federal grant amounts? 3.
What kinds of technical assistance do the MSAP and other ED-funded agencies provide? 4.
What types of assistance are requested? How accessible, useful, and timely is this assistance?
Where Addressed
Chapter VII
Chapter VII Chapter VII Year 2 and 3 Evaluation Reports Chapter VII Chapter VII Chapter VII Chapter VII Chapter VII Chapter VII
Methodology This section presents information about the data collection, interpretation, and analyses that contributed to this report.
The first section outlines the five studies that make up the evaluation and the main data collections associated with each of them.
Several later sections provide more detailed discussions of the data we used to address evaluation questions about desegregation objectives, enrollment trends, staffing characteristics, and student achievement objectives.
Overview of the Five Evaluation Studies Our evaluation features five interrelated strands of inquiry that are described briefly below and summarized in Table A-O-3.
Table A-O-3 Overview of the MSAP Evaluation Studies, Data Sources, and Schedule
Study and Activities Study 1: Profile of Districts Develop database on 57 MSAP projects with information from grant applications and performance reports Interview 57 MSAP project directors Disseminate District Data Request--57 projects Administer Project Survey--57 projects Study 2: Profile of Schools Develop database on 292 schools with information from grant applications and performance reports Administer Principal SurveyÂ­292 MSAP schools Study 3: In-depth Case Studies of 8 MSAP Projects Interview district staff--8 projects Obtain and analyze district-wide standardized test scores and other student achievement data as available
1999Â­2000
2000Â­2001
2001Â­2002
Table A-O-3 (continued) Overview of the MSAP Evaluation Studies, Data Sources, and Schedule
Study and Activities Study 4: In-depth Case Studies of MSAP Schools Interview principal, other school staff Observe classrooms--32 MSAP schools, 15 comparison schools Administer Teacher Surveys--32 MSAP schools, 15 comparison schools Administer Principal Surveys--21 comparison schools Conduct student focus groups--selected schools Study 5: Review of MSAP Guidance and Technical Assistance Interview ED staff who provide technical assistance Issue evaluation reports
1999Â­2000
2000Â­2001
2001Â­2002
Study 1.
Profile of All 57 Projects We are developing a profile of the full population of 57 MSAP-supported districts with descriptive analyses of program context, program characteristics, and enrollment and achievement outcomes.
The profile is based on data extracted from existing MSAP program documents--grant applications and the annual performance reports that grantees submit to ED--as well as through three data collections conducted by the evaluation that are described below.
We conducted telephone interviews of approximately one hour with all of the MSAP Project Directors during fall 1999 and winter 2000.
Consisting of 24 open-ended questions, the interview protocol was designed primarily to identify ways in which existing programs differed from plans described the project's application and to obtain verification of data (e.g., MSAP-supported magnet schools, feeder schools, desegregation goals, and achievement objectives).
We will conduct shorter interviews in fall 2000, to obtain status reports and identify any program changes, and in fall 2001, after the end of the MSAP grant, to determine the status of MSAP projects after federal funding ends.
(See Project Director Interview Guide, 1999Â­2000 School Year, in this Appendix.)
We sent a self-administered Project Survey, consisting of 39 close-ended questions and one openended question, to MSAP Project Directors in late fall 1999.
It focused on student recruitment and outreach, program planning and implementation, accountability, coordination of funding, systemic reform, the role of ED in the MSAP project, and the Project Director's background and role.
Similar,
shorter surveys will be sent again in fall 2000.
(See Project Survey, 1999Â­2000 School Year, in this Appendix.)
Finally, in fall 1999 we sent each grantee a District Data Request (DDR) that asked for information about student, teacher, and administrator characteristics for each school in the district that served the same grade level (or levels) as those served by the MSAP-supported schools during the 1999Â­ 2000 school year.
We sent the DDR to the MSAP Project Directors and asked them to pass it on to their district data managers for processing.
(See the District Data Request in this Appendix for a list of the specific variables that were requested.)
Study 2.
Profile of All MSAP-supported Schools For this study, we focus on the 292 schools1 that receive program funds in MSAP-funded districts: the school context, program characteristics, and enrollment and achievement outcomes.
This profile uses data extracted from MSAP grant applications and performance indicator data provided in the annual performance reports for all 292 schools that have received MSAP support.
In addition, it uses responses to Principal Survey, which was administered to the principals of the 284 schools that were operating MSAP-supported programs during the 1999Â­2000 school year.
The Principal Survey was a 54-item, self-administered instrument that was sent to the principals in late fall 1999.
(In accordance with the wishes of each MSAP Project Director, the surveys were either sent directly to the principals or were sent to the Project Director for distribution.)
Questions focused on features of the school's MSAP program, systemic reforms, accountability, professional development, use and coordination of program funds, the working environment, parent involvement, and the principal's background and role.
A similar, shorter Principal Survey will be administered in fall 2000.
It will feature items on systemic reform and classroom instruction not covered in the first survey and will repeat some items in order to measure changes in the school's magnet program.
(See Principal Survey, 1999Â­2000 School Year, in this Appendix.)
Due to variations in the availability of data for particular analyses, the numbers of schools included in these analyses also varies.
Table A-I-2 summarizes the numbers of programs and schools funded by the MSAP and the numbers of cases included in school-level analyses.
A total of 293 programs were funded by the MSAP.
Because information about student achievement objectives was drawn from grant applications and annual performance reports, analyses pertaining to these objectives could have included all 293 programs.
For reasons outlined later in this Appendix, however, only 289 programs were included.
The 293 programs resided in a total of 292 schools Table A-I-2 also shows that not all of these schools were fully operational during either the 1998Â­99 or 1999Â­2000 school year.
During 1998Â­99, some magnet school facilities were still under construction, and others devoted their first grant year to planning.
Although these "planning schools" may have enrolled students, their special instructional programs were incompletely developed, and recruiting for the magnet program and consequent changes in the proportions of minority and non-minority students enrolled were not expected to occur until 1999Â­ 2000.
By the second grant year, nearly all of the programs had begun operating, but three had been dropped from their district's MSAP project.
The number of cases included in descriptions of magnet schools' enrollment characteristics depends not only on which schools were operating in a given year, but also on the number of these schools for which enrollment data were available from grantees' performance 1
The 57 MSAP projects comprise 293 programs located in 292 schools.
One school contains two small programswithin-a-school (PWSs).
All the other schools are either whole school programs or contain only one MSAPsupported PWS.
reports and/or from the National Center for Education Statistics' (NCES's) Common Core of Data (CCD) electronic files.
Finally, analyses based on responses to the Principal Survey were limited to the 267 schools whose programs were operating in 1999Â­2000 and whose principals completed the survey.
Table A-O-4 MSAP Programs and Schools in 1998Â­99 and 1999Â­2000
Category Programs Funded Programs included in the summary of achievement objectives (based on applications with clarifications from Project Directors) Schools Funded Schools Operating 1998Â­99 Schools Operating 1999Â­2000 Schools Operating both 1998Â­99 and 1999Â­2000 Schools Operating neither year Schools returning the 1999Â­2000 survey *
n 293* 289 292 262 284 261 7 267
The 293 excludes one school that was dropped from a magnet program during budget negotiations prior to the commencement of the grant and includes one school created when an annex of one of the MSAP magnet schools became a separate school.
Study 3.
In-depth Case Studies of Eight Selected MSAP Projects We are developing Case Studies to illuminate the aggregate results obtained from the national data collection (Studies 1 and 2).
Although the case districts and schools were not sampled at random from the full population, the Case Studies will provide examples for the national profiles and will permit comparisons of student achievement outcomes in MSAP schools and non-magnet schools enrolling similar students, within each case district.
During Case Study visits, we are interviewing the MSAP Project Director, recruitment specialist, district curriculum specialist, and any project-level staff funded by MSAP (e.g., resource teachers).
We are also collecting student achievement data from the Case Study districts: students' standardized test scores and other measures such as attendance and dropout statistics, as available.
As shown in Table A-O-5, the eight case projects were selected to reflect the characteristics of the 57 projects considered most salient to this evaluation; all eight projects agreed to participate.
At the request of these Project Directors, the names of the Case Study projects will be identified in this study only as Districts A to H.
The Case Study projects indicates, the 8 projects include populations, location, and size.
intentionally over-sampled states cannot be considered to represent all 57 projects, but as Table A-O-5 both required and voluntary programs and provide variety in student To permit student-level achievement to be examined in depth, we in which such data were likely to be available.
Table A-O-5 Characteristics of 8 Case Study Sites in Comparison to All 57 MSAP Projects
Characteristic Desegregation plan Voluntary Required Average minority percentage in district* Range of minority percentages in districts* Predominant minority group(s): Predominantly Black Predominantly Hispanic Predominantly Asian Geographic region** Northeast Southeast Central (Middle) West No. of states represented State categories*** A states B states C states D states E states F states G states
8 Case Study Projects n % 4 4 8 50.0% 50.0 63.0 32Â­88% 6 2 0 75.0 25.0 0.0
All 57 MSAP Projects n % 31 26 57 54.4% 45.6 61.1 25Â­93% 38 17 2 66.7 29.8 3.5
2 3 0 3 8 4 1 1 1 1 0 0
25.0 37.5 0.0 37.5
17 19 6 15 25
29.8% 33.3 10.5 26.3
50.0 12.5 12.5 12.5 12.5 0.0 0.0
18 8 6 6 15 0 4
31.6% 14.0 10.5 10.5 26.3 0.0 7.0
* Based on Common Core of Data Non-Fiscal Survey (CCD) for 1997Â­98, National Center for Education Statistics ** Based on definitions used by NAEP, NEA, and the Bureau of Economic Analysis of the U.S. Dept. of Commerce.
*** Based on data categories presented at a meeting on student achievement sponsored by the U.S. Department of Education.
These categories are based on the availability of longitudinal student-level achievement data on a state assessment, with "A" states compiling such data and "B" through "G" states compiling progressively less detailed achievement data.
Study 4.
In-depth Case Studies of MSAP Schools Our Case Study selection process was designed to obtain a sample of schools that together would represent all three grade levels and a variety of themes.
Within each Case Study district, we selected four MSAP-supported schools for study.
To help motivate participation and ensure fairness, we invited each MSAP Project Director to choose one school for inclusion in the study.
To the extent possible, we then selected three other MSAP-supported schools at the same level as the Project Director's choice.
We matched the MSAP-supported schools with two comparison schools on the basis of enrollment data from the Common Core of Data Non-Fiscal Survey (CCD) maintained by the National Center for Education Statistics.
The comparison schools identified were non-magnet schools in the district that served students with racial-ethnic backgrounds similar to those in the MSAP schools.
In most cases, close matches were found, but in districts that were small or in which there were numerous magnet schools, the comparison schools tended to have fewer minority students than their magnet counterparts.
Table A-O-6 compares the MSAP-supported schools in the Case Studies and the MSAP-schools in the entire group of 57 MSAP districts.
As the table shows, high schools were slightly over-represented in the sample; middle schools, under-represented.
Table A-O-6 Levels of 32 MSAP-supported Schools in Case Studies and the 292 Schools in All 57 MSAP Projects
Grade Level Elementary Middle High school Combined Levels
32 Case Study Schools n % 18 56.2% 6 18.8 7 21.9 1 3.1
292 MSAP Project n 175 71 40 6
Schools % 59.9% 24.3 13.7 2.1
In April and May 2000, we made site visits to the eight Case Study districts.
Two site visitors went to each site.
They spent one day together in interviewing project-level staff, and then each visited two MSAP-supported schools and one comparison school.
During two-day visits in each school, they interviewed principals, talked with teachers, and conducted classroom observations.
In comparison schools, principals were asked to complete Principal Surveys that paralleled those that MSAP principals had already completed.
In four of the districts, where schools were not all at the same level, additional comparison schools were identified and their principals were asked to complete surveys, to provide additional information about the sites.
Table A-O-7 summarizes the number of MSAP-supported schools visited, and the number of comparison schools visited and surveyed and those surveyed only.
Table A-O-7 Number of Schools Visited and/or Surveyed in Case Study Districts Case Site A B C D E F G H Total MSAP-supported Schools Visited (Principal Surveyed) Elem.
Middle High 3 0 1 3 1 0 2 2 0 0 0 3 4 0 0 2 2 0 0 1 3 4 0 0 18 6 7 Comparisons--Visited (Principal Surveyed) Other 0 0 1 0 0 0 0 0 1 Elem. 1 1 1 0 2 1 0 2 8 Middle 0 1 1 0 0 1 1 0 4 High 1 0 0 1 0 0 1 0 3 Comparisons--No Visit (Principal Surveyed) Elem. 1 1 1 0 0 1 0 0 4 Middle 0 0 1 0 0 1 0 0 2 High 0 0 0 0 0 0 0 0 0
We are developing summary reports that describe in detail four MSAP schools and two comparison schools in each of the eight Case Study districts.
We will make return visits to each Case Study site in spring 2001 during which we will again interview staff and observe classrooms.
During the 2001 visits, we will administer Teacher Surveys in both MSAP and comparison schools, and in some schools we will convene student focus groups as well.
Study 5.
Review of MSAP Guidance and Technical Assistance For Study 5, we are examining the role of the U.S. Department of Education in promoting high quality magnet schools.
We are collecting information about MSAP grantees' needs for technical assistance, the sources they most commonly use, and the quality of the federally supported technical assistance provided to them.
To gain an understanding of the processes and challenges of operating the MSAP program, we have interviewed ED personnel who work with MSAP projects, and are also examining and analyzing documents related to awarding of MSAP grants (e.g., reviewers' comments, unsuccessful applications) and to funds involved in the MSAP awards (i.e., MSAP projects' budgets).
To assess the impact of these processes on the program's clients, we have included items in the Project Survey to elicit grantees' perceptions of the guidance and technical assistance they receive.
Determination of Districts' Desegregation Plan Type To be eligible to receive MSAP funds, districts must be implementing a formal desegregation plan--either a plan that they are undertaking voluntarily or one that has been required by an external authority.
Because the desegregation-related requirements of the MSAP differ depending on whether the district is operating under a voluntary or required plan, many of the analyses in this study disaggregate results by desegregation plan type.
We identified each district's plan type using information provided in MSAP applications.
One of the documents submitted in each application is Desegregation Plan Information (Reference Â§280.20), Part V, on which the district indicates the nature of its plan.
In addition, most application narratives included historical and descriptive information about their plans.
We verified our classifications of the 57 grantees with staff of MSAP and the Department of Education.
Assignment of Grade Level Categories to Schools The content and structure of magnet program differ somewhat by grade level.
For example, high school programs are more likely to focus on vocational preparation than are programs for lower grades, and are more likely to be organized as programs within a school (PWSs) rather than whole school programs.
Consequently, most analyses in this study disaggregate results by school grade level.
Although most schools in the 57 MSAP districts serve conventional grade ranges (kindergarten through grade 5 elementary schools, grade 6 through 8 middle schools, and grade 9 through 12 high schools), there are many variations on the basic pattern.
Some schools open with just a grade or two and phase in additional grades over time.
Others serve wide grade ranges such as KÂ­8 or 6Â­12.
In order to group similar schools together for comparative analyses and to minimize the number of schools in the "other" category, we used the following rules for assigning schools to grade level categories:
Â· Â· Â· Â·
Elementary school: low grade is 3 or below; high grade does not exceed Middle school: no grade is lower than 4; high grade does not exceed that contains a kindergarten as well as grades 6Â­8 is counted as a middle High school: low grade is no lower than 9; high grade is up to 12.
Combined levels school: lowest grade is in the elementary or middle high grade in the high school range (e.g., 4Â­12, 6Â­12, 7Â­12).
8.
9.
One school school.
school range;
Identification of Desegregation Objectives A major legislative purpose of the MSAP program is to assist school districts in reducing, eliminating, or preventing minority group isolation in their schools through the development of attractive instructional programs.
Within the context of the MSAP, "minority" includes individuals of AfricanAmerican, Asian/Pacific Islander, Hispanic (non-African-American), and Native American/Alaskan Native, and a school is defined as "minority group isolated" if minority group students comprise 50 percent or more of its enrollment.
When they apply for MSAP grants, districts identify one or more schools that will be targeted for desegregation impact by their proposed magnet program and specify a particular desegregation objective for each one.
Several of the evaluation questions addressed by this evaluation concern the desegregation objectives that are set by grantees and the schools' success in meeting them.
During the preparatory stage of this investigation, we have collected the basic data needed to identify the schools that will be included in the analysis, as well as the objectives of the schools targeted for desegregation impact by the MSAPsupported project.
In later stages of the study, we will collect and analyze data on the trends in each school's enrollment composition and determine the extent to which grantees were able to meet--or make progress toward meeting--their desegregation objectives.
MSAP grant applications were our primary source of baseline data for this investigation.
As stated earlier, applicants for MSAP support are required to submit detailed explanations of their desegregation plans that contain most of the information we need to establish the framework for our analyses.
In their narratives, applicants must identify the schools in which the federally supported magnet programs will operate (magnet schools) and the schools from which magnet students will be drawn (feeder schools2), and indicate which of these schools are targeted for desegregation impact.3 In addition,
2
For this evaluation, a feeder school refers to the school from which students are drawn to attend a magnet school; that is, a school that "loses" students to a magnet school that serves the same grade level.
This differs from the more common definition of feeder school as a school from which students are drawn for the next level of
they must supply specific information about the desegregation objectives of each targeted school, including enrollment statistics for the year prior to the initiation of the magnet program and projected or benchmark enrollment statistics to be met each year of the grant.
Although most schools and desegregation objectives were clearly identified in the applications, we encountered a substantial number of ambiguities in these documents.
Some applications contained multiple, contradictory statements of their schools' desegregation objectives.
The magnet and feeder school relationships in some districts were more complicated than direct pairings of one magnet with one feeder.
Some objectives did not conform with the federally required goals of reducing, eliminating, or preventing minority group isolation in a targeted school, or were based on variant definitions of "minority" or "isolation"4 For instance, by the MSAP definition, "preventing minority isolation" means maintaining the school's percentage of minority students below 50 percent, but some of applications ascribed the objective of preventing minority isolation to schools whose minority enrollments exceeded 50 percent before the beginning of the MSAP grant period.
Some applications described goals for some of the proposed magnet schools that would have increased their minority enrollments, but increasing minority enrollments in magnet schools is not a legislative purpose of the MSAP.
Since the evaluation questions to be addressed by this evaluation are predicated on federal definitions, we have devoted considerable effort to developing strategies for interpreting and analyzing these locally developed objectives in a manner that is both uniform and faithful to the intent of the MSAP statute and regulations.
In determining each objective, we have considered both the wording of the objective and the nature of the change indicated by the baseline and projected enrollment statistics provided in the application.
To resolve ambiguities encountered during our initial coding, as well as to ascertain that the desegregation plan described in the application did not change before the MSAP project commenced, we also reviewed grantees' first year performance reports and asked each Project Director to verify the information about their schools and desegregation objectives coded in our database.
Finally, we consulted with MSAP, OCR, and OGC staff to resolve the most intractable cases.
Documentation of Trends in School Enrollment The evaluation will use school- and district-level enrollment statistics to describe the context within which the magnet projects operate, to compare characteristics of the MSAP-supported magnets and other public schools, and to document the degree to which each magnet school meets its desegregation objectives.
In order to support rigorous comparative analyses, these data must be:
Â· Â· Â·
collected at the same time each year available for all schools in the district that serve students in the grade level or levels that are served by the district's MSAP magnets disaggregated by minority status as defined by the regulations governing the MSAP, or by the five major racial-ethnic categories used by federal data-collecting agencies
3
4
schooling; for example, the K-5 elementary school which students attend before moving on to grade 6 in a middle school.
Most of the schools targeted for desegregation impact are the schools in which the magnet programs are located.
Sometimes, however, a magnet program targets a feeder school.
That is, the magnet is intended to draw minority students away from a feeder school and thereby reduce, eliminate, or prevent minority group isolation in the feeder school.
Districts with voluntary plans must set objectives for their targeted schools that conform with the federally defined desegregation goals of reducing, eliminating, or preventing minority group isolation.
Objectives set by required plans need not conform to these definitions, but all applicants must explain how their programs support the legislated purposes of the MSAP.
We have obtained school and district enrollment data from four primary sources, each of which involves strengths and limitations.
Data that grantees provide to the MSAP in applications and performance reports document the minority/non-minority composition of the students in magnet and targeted feeder schools as well as district-wide.
These data are disaggregated for grade and minority status and are reported separately for programs within a school (PWSs) where they exist.
Some of the limitations in the quality of the data that grantees provide in their applications and annual reports included enrollment counts that are not from the same time of year in all districts, counts that are from an unspecified time of the year, inconsistent or conflicting counts in the same report, and use of rounded percentages rather than precise counts.
Moreover, the data are generally provided only for MSAP schools and targeted feeder schools, not for all schools in the district.
In addition, a few districts report data in race-ethnic categories different from the minority/non-minority categories that the MSAP is required to track for its performance indicators.
For instance, some districts have required desegregation plans that specify categories of Black/Non-Black.
A few items on the 1999Â­2000 Principal Survey also elicited information about student and program characteristics.
By virtue of their positions in the magnet schools, principals are the best source of information about some aspects of their magnet programs.
However, their information on other topics (e.g., the funding sources for special programs in their schools, whether their school operates a Title I Schoolwide Program) may be limited and at variance with information derived from other sources.
Data provided in response to the District Data Request (DDR) were expected to provide comprehensive enrollment data for all schools in the district, collected in October, and in accordance with the federal minority status categories.
In addition to the racial-ethnic composition of each school, the DDR also asks districts to provide information on the schools' involvement in Title I programs, their students' poverty, mobility, and English proficiency characteristics, and the numbers of students in special education.
Not all districts have been able to comply fully with the request, and data have been provided in widely varying formats and degrees of disaggregation.
Consequently, these data will not support analyses that span all 57 projects.
We are attempting to fill gaps by acquiring data from state databases.
However, this is a laborious process and will probably not yield complete data for all the MSAP districts.
CCD school-level files contain data on the racial-ethnic composition of virtually all public schools in the United States between 1987 and the present.
Thus they are the only source of data available to support analyses of historical trends in district and school enrollment composition.
These data are collected at the same time each year and are reported in the five federal ethnic-racial categories.
For some grantees, the CCD is also the only source of comparable enrollment data for the magnet and non-magnet schools in the district.
The primary disadvantages of the CCD data are the time lag between collection and dissemination (certified data collected in October 1998 became publicly available in September 2000) and the fact that they do not allow analyses of enrollment trends for within-school programs, or for schools that have not been identified by the National Center for Education Statistics (i.e., several magnet schools in New York City).
We have acquired the CCD school-level files for the 1993Â­94, 1995Â­96, 1997Â­98, and 1998Â­99 school years, and will acquire subsequent years of data as NCES makes them available.
In view of the strengths different analyses.
For analyses relying primarily on data from provides an independent source and limitations described above, our study uses different data sources for of desegregation outcomes over the course of the grant period, we are the CCD because it reduces the variability in reporting standards and of data on which to base evaluation of the MSAP programs.
When data
for a targeted school are not available in the CCD, the information provided by the grantees is used to make the evaluation.
For analyses of district context, including trends in racial-ethnic composition of school enrollments extending back to 1993, we are using CCD data for all districts.
Analyses of schools' Title I involvement are based on information from three sources: the CCD,5 the Principal Survey, and responses to DDRs.
For magnet schools, Principal Survey responses were assumed to be correct unless they were contradicted by both the CCD and DDR data.
Data for non-magnet schools could only be drawn from the DDR and CCD.
Availability of Staffing Data for Magnet and non-Magnet Schools In order to address evaluation questions about the differences between magnet and non-magnet school personnel, we attempted to obtain several types of information about school staffs through the District Data Request.
We asked districts to characterize the staffs of each school that served students in the grade ranges served by the MSAP magnets in terms of racial-ethnic composition, highest academic degree earned, credentials held, years of teaching or administrative experience, and proportions with college majors in the subjects they taught.
Many districts were unable to provide the information we requested.
For instance, some provided descriptive statistics for staff in magnet schools only, or for the district overall.
Few could give us any information about the numbers of teachers who had college degrees in the subjects they were teaching.
We are attempting to supplement the information provided by the districts with data from state databases.
Although it is unlikely that we will obtain comprehensive information from all 25 states in which the 1998 MSAP projects are located, we hope that we will obtain sufficient data to conduct analyses of staffing characteristics in several states.
5
For the first time in 1998-99, CCD collected data on schools' involvement in the federal Title I program, whether they operated Title I school-wide programs, and the numbers of students eligible for free or reduced-price meals.
Only a fraction of the states used the new survey form, however, so the CCD does not include school data on these items for all MSAP districts.
Coding of Achievement Objectives and Outcome Data Data Requirements and MSAP Reporting Guidelines In order to describe the objectives adopted by the 57 grantees and to track their success in meeting their goals over the grant period, it was necessary first to identify the objectives adopted by each project and to code them in a detailed and uniform manner.
A completely specified achievement objective consists of several components: the content domain assessed, a description of the students to whom it applies (school, grade level, and occasionally English proficiency or minority status), the nature of the measure, the magnitude of the change in performance that is expected, the time frame within which it is to occur, and a description of comparison groups, if any.
In its guidance to districts applying for 1998 grants, the MSAP informed applicants that some of the information they provided about their objectives had been identified as data sources for annual performance indicators that the Government Performance and Results Act (GPRA) requires the MSAP program to monitor and report each year.
Applicants were encouraged to review the alignment of their local objectives with the MSAP program indicators included in the application materials and expanded upon in an addendum6, These documents provided guidelines for the content and format of project objectives relating to student achievement and the reporting of the results of student assessments aligned with local content and performance standards.
In particular, the outline of performance indicators showed that the MSAP's indicators were built on the assumption that grantees would report the results of its achievement measures separately for minority and non-minority students in each grade within each magnet school or within-school program, that magnet student performance would be compared against a baseline level (to monitor "improvement"), and that magnet student performance levels might be compared to the performance levels of similarly situated non-magnet students or to the district average.7
6 7
DuBois, P.A., J.L. Duff, and E.K. Hawkins.
Magnet Schools Assistance Program (MSAP) Performance Indicators: Guide for MSAP Applicants and Grantees.
Palo Alto, CA: American Institutes for Research, 1997.
The Guide for MSAP Applicants and Grantees states: "In your application, also indicate how you are going to measure changes in student achievement over time (e.g., through growth over time in individual student performance, through comparison of cross-sectional data for students at particular grade levels, or through assessments of students relative to school or district standards) and what measures you will examine (e.g., grades, test scores, ratings of work in portfolios, completion rates, percentage of students meeting State benchmark standards).
Also ...identify relevant comparison or reference groups.
Indicator 4-1 calls for comparisons to other students in the district, by grade and by minority/non-minority status.
If other comparisons are to be used as a part of your evaluation design, describe them as well....
For example, the comparison might be to the scores of non-magnet students in a school that is matched to the magnet school on socioeconomic status and racial-ethnic representation, or to data on State proficiency levels or other specified comparisons.
....In your annual project reports, provide the data described in your application, using the same time points each year (e.g., spring administrations of a State assessment).
Include relevant data covering all students who meet your definition of magnet student (i.e., meet the criteria you established in your application for the purpose of assessing student achievement) and, if applicable, all students in your comparison groups" (DuBois et al., 1997, p. 37).
It should be noted that while these documents provided advice that many applicants found useful in developing their applications, they were guidelines: they did not specifically require that applicants would include particular measures or report their data in a standardized format.
Consequently, the detail and clarity of the achievement objectives described in applications varies considerably, and identifying the objectives pertinent to the MSAP performance indicators has proved difficult in many cases.
Many objectives were stated ambiguously, leaving the reader unclear as to what grades were to be tested, what instruments would be used, the magnitude of the changes required to meet goals, whether the changes were to measured in terms of successive cohorts or the gains of individual students (or cohorts) over time, and the nature of comparison groups.
In addition, it was not always clear that the grantee proposed to report results separately by grade and minority status for some or all measures.
Some applications included varying descriptions of objectives in different parts of the narrative.
Some of the measures described in applications appeared to be informal or "formative" measures that grantees planned to use to inform themselves but whose results they did not intend to present systematically in their MSAP performance reports.
How Achievement Objectives Were Recorded and Clarified The achievement objective database is organized by school.
Each MSAP school currently included in the database has its own set of objectives, often identical to those of every other MSAP school in its district that serves the same grade level.
MSAP applications usually described a set of "generic" objectives that applied to all of the MSAP magnets (or all of the magnets serving a particular grade level).
Sometimes they described somewhat different objectives in different sections of the text.
Schools varied widely in the numbers of objectives ascribed to them, the discreteness of the objectives (i.e., the number of testable conditions embedded within each numbered objective stated in the application), and the specificity with which the objectives were described.8 The task of creating an analyzable database, therefore, was to identify the "official" version of the objectives and convert them into a set of concisely stated goals for each MSAP-supported school.
During our initial coding of MSAP applications, we attempted to create a separate objective record for each element in each compound objective statement.
In particular, when an application was unclear as to whether the project intended to track outcomes separately by subject or as a single composite (e.g., failing grades in courses, content area sub-scores on standardized tests), we coded a different objective for each subject.
And when a grantee indicated that multiple standardized assessments were used by the district, but did not provide details about which grades would be tracked, we assumed that each school would have achievement objectives based on each test for which students in the relevant grade levels were enrolled.
Once we had established a tentative list of objectives for each grantee, we consulted three additional sources in hopes of clarifying and completing the data.
First, we collected information from 8
One reason for the proliferation of objectives in some districts is that the district uses different assessment systems to test different grades.
For example, New York City districts use the state's criterion-referenced test to assess students in grades 4 and 8, and a norm-referenced standardized test to assess students in grades 3, 5, 6, and 7.
Under such circumstances, a general goal for magnet students' performances on standardized tests to improve each year is translated into at least two objectives: one for increases in the percentage of students scoring at a criterion level on the state tests, and one for increases in students mean scores on the norm-referenced test.
Other differences in the numbers of objectives reported arise from grantees' decisions to track results by subject or by a composite.
For instance, some grantees count course failures separately for several subjects (multiple objectives) while others report overall counts of failures in a set of core academic courses (one objective); and some districts track composite scores while others track content area sub-scores.
state assessment program websites to obtain additional details about the assessments used and the grades to which they applied.
Second, to resolve ambiguities in the descriptions of objectives we had found in applications, we consulted grantees' performance reports to see how achievement data were reported for the baseline and first grant year.
Such clarifying information was missing from many first year reports due to conflicts between the MSAP reporting deadlines and the district's testing schedule as well as to recent changes in local assessment programs that rendered many of the original objectives obsolete.
Many first year reports stated that the project would send replacement objectives and/or test results in addenda to their first year reports; a few deferred reporting of baseline and first year scores until their second year reports.
Third, we included questions about student achievement objectives in the Project Director Interviews.
We provided the Project Directors with a listing of their project's achievement objectives as they appeared in our database and asked them to confirm or correct them.
In addition, we included specific clarifying questions about objectives that we had found ambiguous.
Some clarifications are still pending.
Consequently, the database upon which this report is based is a work in progress that will continue to be refined over the next few months a s more information becomes available.
Descriptive Analyses in Chapter VI Despite the challenges in recording the objectives, the information currently available is adequate to describe the nature of grantees' achievement objectives in broad outline.
Each "objective" included in the analysis pertains to one indicator, based on one measure in a particular content or skill domain, for one school.
Because of variability in the grades assessed using particular instruments, individual objectives may apply to one or more grades within the school.
(For instance, some states assess all grades between 2 and 11 in language arts while others test only grades 4, 8, and 10.
In the first state, an objective based on elementary language arts scores would probably involve grades 2 through 5 while in the second state a similar objective would involve just grade 4.).
In some cases, a single measure is represented by two or more indicators (and, therefore, multiple objectives.)
For example, a statement that overall language arts scores will increase from year to year and that the performance gap between minority and non-minority magnet students will decrease over time, is recorded as two objectives.
Because the number of objectives per school varies widely from project to project, a simple reporting of the overall percentages of objectives in various categories would over-represent schools with relatively large numbers of objectives.
Consequently, the analyses in Chapter VI present the proportion of schools that have at least one objective in particular categories rather than the overall proportion of objectives in those categories.
When proportions of objectives are mentioned, they are calculated as the average of the within-school proportions represented by objectives of a particular type.
