A High-Performance Computational Engine for the Analysis of Genomes A Computational Biology Grand Challenge Proposal __________________________________________________________________ Summary The interpretation of the human genome represents the next grand challenge at the interface of computing and biology.
The Human Genome and microorganism sequencing projects will soon be producing data at a rate which far exceeds current analysis capabilities.
Accurate analysis of this data will only be possible with significant high-performance computing resources.
We propose to implement key steps in genome analysis into a high-throughput computational engine capable of meeting the needs of the next phase of genome sequencing.
At the expected sequencing rate, the genome community will be generating between 2 and 20 megabases of DNA sequence every day until the year 2003.
This analysis engine must be capable of analyzing and annotating this time-critical data essentially in real time.
The analysis engine will be created through the integration of high-performance computing resources at the ORNL CCS, LBNL NERSC, ANL CCST, and at other collaborating sites into a seamless distributed computing system.
A consortium of 5 National Laboratories and 6 academic institutions, each with highly relevant multi-million dollar per year efforts will participate in a coordinated program to design and construct this computational system integrated into a problem solving environment.
Accurate analysis of this data will impact not only genome informatics, but experimental throughput at human and microorganism genome centers and related developments in structural biology, genetic medicine, biotechnology, pharmaceuticals, and bioremediation.
Throughout the project there is a need for state-of-the-art computing, efficient parallel implementations of a diversity of new algorithms, dynamic and seamless management of contiguous distributed high-performance computing processes, complex distributed data mining operations, and the design and application of new inferencing and visualization methods.
Additionally, the richness of the analysis which must be done, the critical long term need for this analysis, and the size of the collaborative community, make this problem uniquely challenging from a computing point of view.
I. SPECIFIC OBJECTIVES AND GRAND CHALLENGE CRITERIA
The goal of this work is to design, develop and implement a distributed high-performance computing-based problem solving environment for real-time processing and interpretation of massive amounts of time-critical genomic sequence data.
Our specific aims are as follows:
* Implement the basic components of a distributed problem solving environment consisting of three software layers for the management of the analysis process.
* Develop and implement portable scalable high-performance computer codes for pattern recognition, homology search, statistical sequence measures, gene modeling, and gene assembly state calculation within an analysis engine.
This engine will require very significant compute power primarily from the participating supercomputing centers.
* Develop effective methods of automated Internet data capture for input to the engine and an effective data mining component to retrieve remote information relevant to the course of analysis.
* Implement an efficient warehousing system using the available terabyte storage facilities, to contain intermediate results of the analysis engine and contain output for user visualization and distribution to central community databases.
* Develop advanced visualization interfaces to view the complex analysis results and links to related data in remote resources.
Particularly, the proposed Grand Challenge Application will address the following criteria:
i. Fundamental significance
This proposal will satisfy the need to provide automatic annotation for DNA sequences generated by a number of centers and from multiple organisms at a rate of 2-20 megabases(Mb) /day.
This will have direct impact on medical and biotechnology research which account for billions of dollars per year in the U.S. economy.
ii. DOE Mission
This Grand Challenge (GC) proposal is critical to the missions of the Human Genome Program and the Microbial Genome Program within OHER, and has broad implications for programs in bioremediation and health assessment, and provides for significant computing advances applicable to other disciplines within DOE.
iii. HPCC Goals
The Human Genome Project is at the heart of the future of health and biological research.
Only through two orders of magnitude increase in computational analysis capability will the fruits of this large-scale experimental program be realized.
The project is responsive to the priority goals in (1) having direct impact on the health and well-being of US citizens; (2) developing new computing and communication technologies and (3) defining new mechanisms to provide the service of these technologies to answer important health and biology questions.
iv. Enabling Technologies
The collaborators and the facilities within this proposal represent the technologies needed for rapid development within genome sequencing, including expertise in genome analysis tools, parallel algorithm design, heterogeneous distributed computing, data management, advanced visualization, web-based collaboration technologies, and facilities with terabyte capacity storage, high-performance computing systems, and high-speed networking.
v. Interdisciplinary Approach
The project includes a diverse group of biological scientists of many types (genome scientists, molecular biologists, developmental biologists), statisticians, computational and computer scientists, and software engineers from the proposing laboratories and a large number of collaborators from academia and non-profit research institutions.
vi. Support leveraging
This Grand Challenge represents a leveraging of significant activities within the OHER Genome Program, and significant new OHER funds directly related to matching this effort are expected starting in FY 97 (see OHER support letter in Appendix I).
The automation and high-throughput aspects here are a significant expansion of individual OHER funded projects.
The infrastructure activities represent a leveraging of existing activities within the MICS and AMS programs.
The Computational Center for Industrial Innovation (CCII), a DOE National User Facility offers a mechanism for leveraging this project with the industrial community.
vii.
Technology Leveraging
This project will make extensive use of existing technologies from which to build the proposed Problem Solving Environment (PSE) including expertise in message-passing, I/O libraries, scalable algorithms, user interfaces, schedulers, etc.
The specific goal of this proposal, i.e., the analysis engine to analyze raw sequence data, will be targeted only to this application.
Previous experience in Grand Challenge and collaborative technology applications within the Collaborative Management Environment will be used in this work.
Also, the underlying technologies that will be developed for this project are very well suited for other application areas, e.g., tools for developing platform and architecture independent software, communication and management tools that build and control a distributed collaboration, and software agents that feed information from the Internet to the terabyte storage facilities.
viii.
Computer Resources
The Intel Paragon platform at ORNL, the Cray T3E at LBNL, and the ANL CCST SP2 will be the primary architectures.
In addition we will develop software to use the Parallel Distributed Systems Facility (PDSF) as an interim solution before moving on to the more powerful SMP cluster scheduled at NERSC.
Peripheral large storage requirements are also met at these facilities.
Clearly, the distributed activities require networking support, and we will work with staff of ESnet headquartered at NERSC.
The supercomputing centers will provide support staff specifically for parallelizing the algorithms developed by the collaborators and consulting on various issues regarding networking, collaboratories, and hardware resource management.
ix. Multiple Platforms
Due to the large and daily demand for cycles, we assume at the outset the need for a flexible task brokering system capable of using multiple types of platforms located at multiple sites.
The proposed analysis engine will be composed of a suite of tools that make use of flexible libraries to achieve portability and optimal performance on both shared and distributed memory MPP systems represented by the ORNL/CCS, LBNL/NERSC, and ANL/CCST facilities.
The use of workstation clusters like the PDSF at LBNL and DEC cluster at ORNL CSMD will further ensure portability.
A number of the PIs have either been involved in porting software to multiple architectures or in the definition of high-level abstraction to parallel processing problems.
The CCS, NERSC, and CCST are committed to development of portable, scalable applications for this analysis engine and have significant experience with a variety of platforms in addition to the Paragons and T3E.
Oak Ridge has long been a leader in heterogeneous distributed computing, and current research into seamless computing environments will be applied to the analysis engine.
MPI and PVM are installed, and have been used to demonstrate the portability of codes from the Paragons to other platforms.
II.
RATIONALE
The recent funding of more than 10 major genome centers to begin community-wide high-throughput sequencing of the human genome has created a significant new challenge for the computational analysis of DNA sequence and the prediction of gene structure and function.
It has been estimated that on average from 1996 to 2003, approximately 2 million bases of newly finished DNA sequence will be produced every day and be made available on the Internet and in central databases.
In addition, sequence will be released for a given clone well before the final sequence assembly, in the form of smaller sequence fragments (contigs) down to 1 kilobase (kb) in size.
This means that at any given time, a much larger amount of partially assembled sequence will need immediate analysis.
We estimate that on average between 2 and 20 million bases of sequence will need to be analyzed each day, and the state of these sequences will be dynamic, changing day to day.
With only a small fraction of genes which cause human genetic disease identified, each new gene revealed by this analysis has the potential to have significant human health impact, and in total within the sequence are genes with direct impact on the diagnosis and treatment of some 6000 human genetic diseases.
Timely development of diagnostics and treatments related to these is worth billions of dollars for the US economy, and computational analysis is the foundation which can supply the necessary knowledge for this.
The finished (fully assembled) sequence generated each day will represent approximately 75 new genes (and their respective proteins), and many times this number will be represented in partially completed sequences.
The information contained in these is of immeasurable value to medical research, biotechnology, the pharmaceutical industry and researchers in a host of fields ranging from microorganism metabolism, to structural biology, to bioremediation.
Sequencing of microorganisms and other model organisms with significance for biotechnology, bioremediation, and the pharmaceutical industry is also ramping up at a very rapid rate.
The genomes for H. influenza and yeast have just been fully sequenced, although the significance of many genes remains to be determined.
The potential for the discovery of new enzymes and chemical processes important for biotechnology (for example - new types of degradative enzymes) as well as new insights into disease causing microbes makes these efforts highly valuable in both economic and social terms.
In addition, information derived from the computational analysis we propose can also make the sequencing effort more efficient and cost effective at the genome centers.
The key to producing biological information and insight from this flood of data lies in effective and timely computation.
In the last year or so new types of experimental information have become available which can be used to produce very accurate analysis of genome sequence data.
Algorithmic methods to use this information have lagged behind, but are just now becoming available.
These methods require computational power unprecedented in the analysis of genome data.
The recent computational analysis of the Haemophilus influenzae and Mycoplasma genitalium used relatively simple methods, required months of computing effort, and yet this sequence represents less than one day's data in the upcoming phase of the genome project.
Unless a more efficient approach and far more powerful computational engine is constructed for this analysis, accurate large-scale analysis of genomes will be virtually impossible.
The interpretation of genomes represents the next grand challenge at the interface of computing and biology.
The analysis we propose is both time-critical and extremely valuable in scientific, social and economic terms.
The extremely large datasets generated experimentally will thoroughly test the capabilities of the current generation of supercomputers and meeting this challenge will require cutting edge computing technology and resources.
By scaling the component calculations for a MPP environment, by using sophisticated intelligent agents for distributed data mining, by tuning the aggregate clusters' I/O with a massive storage system, and by creating a high information content visual interface to the combined processes, we expect to be able to develop a solution for the entire research community.
Only through this intensive Grand Challenge effort can an analysis engine be constructed capable of meeting this critical computational need.
III.
BACKGROUND AND SIGNIFICANCE
The human genome project, supported by DOE and NIH, is revolutionizing biology and biotechnology, providing an important thrust to the growing importance of the biological sciences and their practical applications.
The increasing ability to analyze, manipulate, modify and create biological macromolecules creates new knowledge about fundamental biological processes as well as numerous opportunities to apply this knowledge.
There is significant new capitalization in the US, as companies new and old, seek to utilize the information for medicine and health care that stems from knowing all human genes, and the sequences of DNA from many humans.
In addition to health-related biotechnology, other application areas of great importance to DOE include bioremediation, waste control, meeting energy fuel demands of the future and health risk assessment.
In the next three years, almost a dozen genome centers in this country as well as in Europe and Asia, will be sequencing human DNA at a rate of at least 2 Mb/day finished sequence.
Centers have agreed that their data will become available very rapidly, often even before the actual data gathering and assembly of each sequence domain (clone) is complete.
The rapid scale-up in the rate of human sequencing, from its current level of about 10 million base pairs/year has been made possible by a significant investment in engineering developments over the last 5 years.
Unfortunately, a concomitant development in the technology to analyze and annotate this data has not occurred.
Sequencing of microorganisms and other model organisms is also ramping up rapidly.
The net effect is to have a product, for which the US government is paying approximately $200M/year, that cannot readily be utilized.
This genome analysis Grand Challenge application is the response to this need.
Annotation is the essential prerequisite before genome data can become useful.
The plain sequence is a meaningless string of characters.
At a minimum, this data must be annotated to indicate the existence of gene coding regions, control regions, and relationships between the sequence and other known sequences.
Equally important are statistical patterns such as the distribution of G+C content, codon usage, and correspondences between computed biochemical properties and properties of expressed proteins measured in the wet-lab.
Beyond coding region identification, finding features like short repeats and long repeats, characterizing the organization of promoters and coding regions, and tying together evidence for metabolic pathways are further annotation activities that add knowledge and value to a genome.
As complete genomes, on the order of 2 to 3000 Megabases are sequenced, the length of DNA comparison strings has changed from single genes to entire genomes, with a concomitant expansion in the time to compute.
In order to look at long range patterns of expression, syntenic regions on the order of 10's of megabases become of reasonable length for consideration.
Significant work is required to develop data management systems to make the data usable as input to computation and amenable to complex ad hoc queries.
In the long run, biologists will want to be able to integrate data obtained at different scales and with different uncertainties and formulate queries against such heterogeneous data.
This will require improvements in data models, heterogeneous database management systems, multivariate correlation analysis, molecular structure prediction, constrained-network modeling and uncertainty management.
The work proposed in the Grand Challenge application will involve DOE genome centers at LBNL, LLNL and LANL and the ongoing programs in computer-based DNA sequence analysis at ORNL and ANL.
Each of the participating National Laboratories and other institutions have multi-million dollar efforts in experimental genomics and related computing for the analysis of genomes.
Each also brings a host of algorithms and methods which are relevant to this proposal, including algorithms for pattern recognition, homology calculation, and unique data resources.
Despite the size of this collaboratory, this group of participants has had significant previous experience in jointly addressing significant community research problems.
The analysis results obtained in this project feed back to experimental studies at the Genome Centers in many important ways.
Information about genes and fragment assembly provide the Centers with information about the clones they are attempting to sequence in the laboratory, potentially making sequencing and the assembly of sequence more efficient at the experimental level.
In genomic sequencing, closing gaps in the sequence and reaching ``closure'' of the sequence assembly for large genomic clones is the most time-consuming step of the sequencing process.
The information provided in this computational analysis could very directly make this closure process more efficient, saving significant time and limited experimental resources.
The computation in this proposal will very effectively help in the interpretation and use of EST data that has been experimentally collected community-wide at a cost of many millions of dollars.
We will gain important insight into the efficacy of EST sequencing and learn about potential experimental artifacts or shortcomings in the use of EST sequencing.
Many research centers have experimental programs to understand the biochemical function of genes, whole organism gene function and regulation, and the basis of human health and disease.
All of these programs will directly benefit by the added understanding that the computational analysis in this project will provide.
It is the post-sequencing analysis of genomic DNA which justifies the sequencing.
The expectation is that this investment will lead to a better understanding of how cells and organisms work, how genetic diseases can be treated, and how micro-organisms can be harnessed for new biotechnological applications.
There is a huge gap between the string of characters representing the DNA sequence and the ability to query a knowledge-base about an interpreted genome.
Researchers in the field are overwhelmed with the vast amount of data being generated and find it impossible to deal with these in the traditional ways.
This proposal represents a new high-throughput approach for dealing with the volume and complexity of these data so that the community gains the essential insights needed as a result of the genome sequencing efforts.
IV.
APPROACH
The Analysis Engine
To meet the processing challenge posed by this data, we propose to design and implement an analysis engine which will integrate a suite of tasks on multiple high-performance computing resources.
In addition to the need for state-of-the-art computing resources, this engine will require dynamic and seamless management of contiguous distributed high performance computing processes, efficient parallel implementations of a number of new algorithms, complex distributed data mining operations, and the design and application of new inferencing and visualization methods.
The richness of the many types of analysis which must be done and the long term need for this analysis, make this problem uniquely challenging.
The analysis engine will be implemented using an approach based on the implementation of a Problem Solving Environment (PSE) (Gallopoulos et al. 1994, Rousselle et al. 1996, Adve et al. 1994).
The data flow for this grand challenge project is given in Figure 1.
[gen-1.gif] Updates to raw experimental sequencing data will be retrieved through the use of intelligent agents and stored in the local warehouse.
Most human genome centers will daily post new sequence on publicly available ftp sites and we will work closely with each center to establish mutually agreed upon processes to capture their data.
This data will feed the analysis engine which will return its results to the warehouse for use in later analysis processes, visualization by users, and distribution to community databases and genome sequencing centers.
All of the above data flow processes will strain the bandwidth capabilities of both the network and the supercomputer's I/O.
The analysis engine we propose to create combines a number of pattern recognition, statistical measurement and data mining processes into a coherent computational engine running on distributed high-performance computing hardware.
A schematic of these processes is shown in Figure 2.
[gen-2.gif] A process manager will conditionally determine necessary analysis steps and direct the flow of tasks to massively parallel processing (MPP resources) at the CCS, NERSC, CCST, and other facilities.
These processes will include multiple statistical and AI-based pattern recognition algorithms, computation for statistical characterization of sequence domains and sequence comparison and homology calculation systems implemented on MPP hardware and capable of running at multiple sites, as well as algorithms for modeling the structure of genes and the calculation of sequence assembly state parameters.
The Process Manager / Task Broker will also initiate multiple distributed data mining processes which will access remote (often site-specific) databases for information of potential relevance to the genes and other features discovered in a particular sequence region.
These intelligent data mining agents will direct the course of the retrieval, interpret retrieved data and integrate retrieved information with the calculations and decision-making processes.
To meet the analysis challenge of this project, significant computational advances will be needed in six core technology areas represented in Figures 1 
(1) Seamless High Performance Computing
Megabases of DNA sequence being analyzed every day will strain the capacity of existing supercomputer centers.
A number of highly parallelizable computations will be implemented at multiple sites and will need to rely on sophisticated resource management tools within a problem solving environment for maximum efficiency.
Interoperability between high performance computing centers will
provide the aggregate computing power required.
Fault tolerance will need to be addressed to ensure that all data is processed even in the face of network or processor dropouts.
I/O will be a major concern in this project as the input sequence information and metadata is retrieved from the warehouse and distributed to the components of the analysis engine.
In addition, intermediate results will need to be transferred between processors during the course of the many calculations and as results are fused from the tasks and returned to the warehouse.
The intense I/O requirements of this analysis will need to be researched to achieve the greatest efficiency for the overall calculation.
All codes must be adapted for portability and efficiency in both shared memory and distributed memory configurations represented in the aggregate computer cluster offered by the three participating high performance computing centers.
In many cases the same task may be carried out at a number of locations depending on processor availability (and based on load-balancing considerations).
Libraries to insulate the research collaborators will be integrated into a problem solving environment.
This PSE will provide the computational tools researchers will need to develop technology for the intense analysis of genome data.
(2) Parallel Algorithms for Sequence Analysis.
The next generation of genome analysis will require the integration of algorithms into an analysis engine capable of automated processing, to achieve the necessary throughput and to eliminate the need for time-consuming manual trial and error gene model building.
Six distinct types of algorithms (pattern recognition, homology, statistical, gene modeling, sequence assembly and data mining algorithms) will need to be combined into a coordinated toolkit to synthesize the complete analysis.
One area where significant scalability issues will need to be addressed is in the processing of the expressed sequence tags (ESTs are experimentally obtained gene message fragments).
Examining the relationship between 20 million bases of DNA and 1 million ESTs (average length of 500 bases each) typically involves calculation of a score matrix of 1016 elements.
This single calculation using the standard FASTA algorithm requires a 60 hour calculation at a sustained rate of 40 gigaflops.
Comparing potential coding exon candidates in a 20 million base sequence against the Genpept protein sequence database using FASTA would typically require an additional 10 hours at 40 gigaflops.
More sensitive algorithms for this, which also consider possible sequence errors, would require many times more computing.
Furthermore, we will evaluate several different types of algorithms, some with significantly greater complexity than FASTA, to evaluate related EST and protein information.
The number of data resources that must be searched is also large and the amount of data in them is increasing exponentially.
Each EST and protein resource must be searched every day and this is only one facet of the computation needed in the engine outlined in Figure 2.
A variety of pattern recognition systems will also be included in the engine.
Many of these such as the ORNL GRAIL coding exon finding system and the ORNL Gene Modeler (in development) have a hybrid structures which involve a calculation of a set of individual statistical tests for each sequence region, followed by an integration of the results of these tests using a neural network or other fusing mechanism for final prediction or scoring of gene model features.
Most of the computing time is spent in the statistical tests and some are much more computationally intensive than others.
Again the potential advantages of a scalable parallel solution is significant.
Scalability and interoperability issues will need to be addressed for the full range of algorithmic components within the analysis engine, including (1) advanced pattern recognition systems which predict a host of sequence level features (gene promoters, repetitive DNA elements, etc.) based on a fusion of results from multiple (potentially concurrent) statistical tests, (2) algorithms to characterize the long-range statistical properties of genomic sequence regions and isochores, (3) algorithms to combine protein homology, EST databases and pattern recognition information in the initial recognition and modeling of genes, (4) new algorithmic methods to construct models of partial genes, genes which cross gaps between contigs, genes with exons missing in gaps, (5) algorithms to automatically construct models for multiple genes in a single sequence domain through the integral use of pattern recognition and homology information, (6) methods to calculate order in unordered contigs using gene models and homology information, as well as methods to recognizing genes which span between contigs, (7) calculations to collapse EST assemblies based on genomic gene models, and (8) methods for selective use of coding indel detection and frame correction calculations in low quality sequence regions.
Significant effort will be applied to this entire set of algorithmic codes to achieve portability, scalability, and the orders of magnitude speed up which will be required to make these efficient in the context of the combined analysis engine.
(3) Intelligent Agents for Data Extraction and Mining.
Many types of information with potential value for interpreting each newly sequenced genome region must be retrieved from more than a dozen remote databases.
Automated methods to retrieve, collate and fuse database information will be implemented to provide insight into a gene's biochemical function, pattern of expression, organism function, the corresponding protein's potential structure or structure class, functional family, functional motifs, metabolic role, and potential relationship to disease phenotypes.
A toolkit of intelligent agents for fast data search and retrieval will need to be developed for the disparate database systems in which this data is stored.
These tools will need to be modular, to provide for the greatest reusability between databases, and be very robust in the face of the many types of data errors which occur in these databases.
The agents will need to select and retrieve only relevant data, allowing for fault tolerance for temporary network or database dropouts.
The agents also will need to be intelligent to handle variation in the underlying data structures, use inferencing to combine facts into higher-level knowledge and affect overall process flow decision-making.
(4) Data Warehouse.
The data retrieved by the intelligent agents or calculated by the analysis engine must be efficiently stored in the local repository for optimal access by the analysis engine.
The terabyte storage facilities at the CCS and NERSC will be used to handle the data warehousing needs of the project.
The data model must allow retention of the location of the distributed source of the data, the results of analysis, and pointers to locations within external data sources relevant to the analyzed sequence.
Some of these data sources include multimedia (gene expression and anatomical objects which involve multidimensional data).
The data warehouse component of the project will leverage components developed in previous Grand Challenge efforts and will need to optimize the access times by both the analysis engine itself and by mining agents seeking specific calculated results.
(5) Visual Collaboratory User Interface.
The volume and complexity of the analyzed information and results will require an extremely sophisticated approach to access and visualization.
Users need to interface to the raw data, the analysis process, and to the resulting synthesis of gene models, features, patterns, map data, anatomical phenotypes and other information.
A number of visual code development and process monitoring tools will be integrated within a primary user interface.
This environment will incorporate collaboration tools such as conferencing and electronic notebook technologies.
Finally it will create data visualization tools to interface with the data retrieval agents, the analysis engine, and the data warehouse.
A modular toolkit will provide the data communication framework for user developed views to be created or adapted to this analysis process, leveraging the ongoing efforts within the community.
This will provide the needed protocols for a full collaboration environment among genome researchers.
V. RESEARCH PLAN
Megabases of DNA sequence being analyzed every day will strain the capacity of existing supercomputer centers.
A number of highly parallelizable computations will be implemented at multiple sites and will need to rely on sophisticated resource management tools for maximum efficiency.
Interoperability between high performance computing centers will provide the aggregate computing power required.
To address the critical computing infrastructure needs of this domain, a Problem Solving Environment (PSE) (Gallopoulos et al. 1994, Rousselle et al. 1996, Adve et al. 1994) will be created whose core is an analysis engine composed of tools for genome analysis.
This computational engine will require cutting edge solutions in a significant number of computational technologies encompassing seamless computing, new scalable and portable algorithms, intelligent agents, data mining, data warehousing, visualization and collaborative tools.
To establish the computing infrastructure for this problem, a PSE will be developed specifically for the genome domain, to integrate the needed tools for developing the analysis engine, and to provide the framework for extensibility to future tools and techniques.
These two sometimes conflicting criteria will be handled by the use of a three-tier model for the infrastructure of this environment.
This modular approach will insulate the user from the details of the specific MPP implementations, and provide the interface descriptions for future tool development.
A schematic of this model is shown in Figure 3.
[gen-3.gif] To facilitate the use of these technologies in the genome community requires that our basic problem solving environment relieve the research scientist of the burdens associated with computer science and applied mathematical issues.
To develop computer codes that are portable, heterogeneous, fault tolerant and highly efficient would normally require a genome scientist to be knowledgeable in high level computer science and applied mathematical areas, and this need has led to an inefficient use of time and the slow migration of genome analysis tasks to high performance technologies.
We expect that the environment we propose will not only greatly accelerate the progress of this project's objectives, but also provide a framework for continued progress throughout the entire genome research community.
In addition, the base PSE system we will construct for genome analysis can be used as a toolkit for building other grand challenge application environments.
Major challenges for the construction of this environment are efficient inter/intra message passing, portable I/O libraries for parallel and serial file systems, portable multi-threading libraries, distributed visualization, and Java browsers and interfaces.
These will be discussed in more detail in subsequent sections.
The development of algorithms that are portable, heterogeneous and fault tolerant represents a major task because an algorithm designed on one platform is not necessarily the best algorithm on another, as well as the possibility that neither is optimal for a heterogeneous environment.
We assume at the outset that resources from CCS, NERSC, CCST and several other sites will be used to meet the demand for computing power, and any of a host of algorithms must be able to run at any of a number of locations.
In addition, the suite of algorithms in the analysis engine requires the exchange of data, internally within parts of algorithms and also between algorithms running simultaneously (and potentially at different sites).
The improved ability to communicate between task processes which are directly relevant to one another will allow a much more sophisticated analysis plan to be implemented in the genome analysis engine than has been previously possible.
In current implementations of genome analysis systems, these tasks are executed sequentially, and important relationships between different parts of the analysis are not fully utilized.
Additionally within the framework provided by the base genome analysis PSE, are major computer science challenges such as the development of genome analysis algorithms with improved scalability, new algorithmic designs to take advantage of inherent function and data decomposition, portable algorithmic design and optimization, distributed databases (including the use of high performance storage systems for intermediate and final genome analysis results), and advanced methods for visualizing analysis processes and results.
These implementations represent the central core of this Grand Challenge.
We summarize these challenges for the genome analysis engine as follows:
* New algorithmic design to take advantage of inherent MIMD structure * Portable algorithmic design and optimization * Order N3 or higher complexity algorithms (like gene modeling) to lower order or potentially order N algorithms * Base PSE design and implementation * Efficient intra/inter message passing * Multi-threading on shared and shared/distributed machines * Distributed data warehousing and high performance storage systems * Distributed visualization * Java and JavaScript browsers and interfaces
We conceptually divide the problem into five research and development tasks:
* Task 1.
The Problem Solving Environment * Task 2.
Parallel Algorithms for Genome Analysis * Task 3.
Data Mining and the Data Model * Task 4.
Data Warehousing * Task 5.
Visual Collaboratory User Tools
In the following sections, we provide a more detailed description of the structure of these tasks:
Task 1.
Seamless High Performance Computing in a Problem Solving Environment
To ensure portability and interoperability of our computer codes and to facilitate the use of high performance technologies in the scientific community both in developing and using codes for the analysis of genomes, we will implement the basic units of a high-level Problem Solving Environment (PSE).
This PSE which will support fault tolerance and task migration, and will ultimately provide a software library containing the basic tools that will allow other application developers in other disciplines to develop their own environments.
The main principles of a coherent environment are the integration of different tools and process stages and interfacing with the user at the user's level of abstraction.
Tools to be integrated into a parallel environment are the editor, compiler, runtime monitor, scheduler, performance analyzer and debugger.
These tools interface to the hardware and operating system software at the message passing level but communicate with the applications developer at the data parallel source code level.
In addition, the PSE must integrate tools used throughout the problem solving process.
The PSE should also provide a virtual computational and information processing system that is closer to the genome user's problem than to general purpose parallel hardware and systems software.
The environment used to develop the software components is envisioned as consisting of three independent software layers which are based on object-oriented methodologies, referred to earlier and in Figure 3.
The object-oriented methodology provides the flexibility to extend this environment to a more sophisticated PSE (perhaps for later Grand Challenge problems).
The top layer consists of a set of tools that allow the user to edit or modify an existing code, to set up all aspects of a numerical simulation, to follow the progress of the calculation, and to visualize the data.
This contributes significantly to the ability to integrate codes created by many different genome and computer science researchers.
The middle layer consists of services for coordinating the top and bottom layers, for job compilation, submission, scheduling, task migration, data management and monitoring.
The bottom layer consists of portable software libraries used for the numerical simulation, including highly optimized message-passing, fault-tolerance, and mathematical software libraries and distributed visualization.
A major goal of this GC proposal is the development of portable, heterogeneous libraries that hide the intrinsic details of particular resources on high performance systems, as well as presenting a coherent interface to the application developer.
This drives the concentration of efforts for this PSE to the bottom layer where the libraries reside.
Many features of the top and middle layer can be leveraged against other projects and software, with the remaining components requiring little development or only minor modifications to existing software.
This additional leveraging will guarantee a successful completion of this project as well as the delivery of an early prototype system within the first year.
This is essential to keep up with the rapidly growing flow of genome data, and will provide the research team with the necessary software tools to accelerate the overall analysis process within the genome community.
In the following sections a succinct description of the top and middle layers is provided, followed by a more detailed view of the bottom layer:
The PSE Top Layer - Interfaces
Interoperability is a significant impediment to both software reuse and to collaboration generally, and specifically within the genome informatics community.
Within the PSE, the top layer will integrate existing software tools into a coherent primary user interface using Java (van Hoff et al. 1995), and JavaScript (Kent and Kent 1996) for maximum accessibility.
This primary interface will be able to execute other applications.
A Java front end will be prepared for the Distributed Source Control (DSC) package (Rowan and Reed) to support collaborative development.
Pre-existing programming editors will be used initially, but as Java editors mature they will be integrated into the system.
HeNCE (Dongarra et al.) and XPVM (Geist and Kohl 1995) will be leveraged for this project to provide program development and analysis tools, respectively.
As part of the top layer we will provide visual software specifically tailored to the genome community (described below in Task 5 - Visual Collaboratory User Interface).
In addition to code development and data analysis systems, we will leverage ongoing efforts in electronic notebooks (see Electronic Notebooks Workshop reference) and video conferencing to provide collaborative tools.
An electronic notebook is a medium in which researchers can record aspects of collaborative experiments that are conducted on expensive, hard-to-reproduce facilities.
But use of electronic notebooks goes beyond just documentation of remote instruments.
They can be used as private notebooks that document personal information and ideas, or a single notebook shared by a group of collaborators as a means to share scientific ideas among themselves.
The genome community has had significant experience with the use of electronic notebooks.
Access to a particular electronic notebook, its contents, and authentication of entries are fundamental security issues that any notebook must address.
In a project at ORNL (Geist, 1996) every notebook has a list of authorized users.
Thus privacy is handled at the notebook access level.
If it is a list of one, then it is a private notebook.
If the list is greater than one then it is a group notebook set up for remote collaboration.
To provide tamper-proof entries, the design of the notebook automatically dates and digitally signs each entry with the users PGP signature, then appends the entry to the last page of the notebook.
All submissions across the Internet are encrypted with a session key that is created during the authentication phase of notebook access.
Electronic notebooks, just like the notebooks that sit on researchers desks, are shared resources and as such require interoperability and portability.
The rapid development of web-based communication technologies will be leveraged for the primary user interface.
These collaboration technologies are especially important considering the size, Internet bandwidth, and distributed nature of this collaborating group.
The main thrust of work in the top layer will be on the issues of integration of existing and emerging technologies.
The PSE Middle Layer - Services
The middle software layer handles the connectivity between the top and bottom layers within the object-oriented methodology of the PSE.
This layer handles the scheduling of tasks, communication of data, retrieval of code libraries, and distributed source control.
Schedulers.
To handle the scheduler needs within this PSE, we will leverage the existing work on CONDOR (see CONDOR reference) which is based on PVM. CONDOR is a distributed resource management system that can manage large heterogeneous clusters of workstations.
Its design has been motivated by the needs of users who would like to use unutilized capacity in such clusters for long-running, computation-intensive jobs.
CONDOR has been ported to most UNIX platforms and we will extend CONDOR to high performance computing systems.
Data Management.
Data management will also be provided through the consideration of two middleware systems.
The first is for data management between the analysis engine and the data warehouse.
We will leverage the existing Collaboration User Migration Library for Visualization and Steering (CUMULVS - Papadopoulos and Kohl, 1995) to provide for the data management needs within the high performance computing systems.
CUMULVS is a software infrastructure for the development of collaborative environments.
It supports interactive visualization and remote steering of distributed applications by multiple collaborators, and provides fault tolerance to applications running in heterogeneous distributed environments.
Specifically, CUMULVS provides:
* Collaborative on-line visualization of remote simulations through multiple viewers connecting to a running application * Coordinated computational steering among collaborators * Automatic recovery of the virtual environment to host or network failures * Fault-tolerance of distributed simulations through heterogeneous task migration and user-directed checkpointing * A monitoring system for high-performance scientific simulations * Dynamic viewer attachment / detachment * Message-passing over wide-area ATM networks between multiple MPP and SMP systems * Secure and authenticated data transfer
A second data management protocol will be considered for use by the intelligent agents providing for retrieval and communication of data objects with remote databases and the user community.
CORBA (see CORBA reference) is a comprehensive distributed object-oriented environment.
Data models are represented using CORBA's interface definition language (IDL) which includes a specification of the methods or operations available for the object types (classes).
Object request brokers (ORBs) serve objects according to the specification and are not constrained to any language or platform for implementation.
Raw data residing in a DBMS are retrieved by the method implementation.
A major advantage of this environment is the responsibility for implementation of the methods associated with a particular class lie solely with the site serving it.
Client users need no knowledge of implementation details.
A mediator between these two data management systems will be evaluated for efficiency, ease of use and portability.
Code Agents.
As the goals of reusability for code and heterogenous software libraries are met, the need for retrieval agents will rise sharply.
Intelligent agents for the retrieval of code libraries will provide the means to efficiently access only the libraries that are needed in a given application.
These agents will provide the wrappers to legacy code libraries to function in the same paradigm as Java applets, to be retrieved on the fly upon demand.
Help System Agents.
The PSE will provide translation agents for underlying help subsystems, including man pages, application code write-ups, library descriptions, etc.
Translators will be integrated into the user interface to provide seamless access to these disparate information sources.
Existing translators such as man2html (Niwinski) will be used when possible to produce a complete, seamless, web-based user help information system.
Search and retrieval agents will be leveraged from developments in the Collaborative Management Environment.
Distributed Source Control.
Code development within this large collaboration will be facilitated through the maturity of the Distributed Source Control package (Rowan and Reed).
The primary design criteria of the Distributed Source Control (DSC) approach is to provide software developers easy access to the benefits of source control.
DSC is built upon and is compatible with the Concurrent Versions System (CVS) source control program (Cederquist).
DSC provides users a set of simple source control commands to interact with a DSC server running at the site of a remote master source repository.
DSC controls CVS operations at this remote host.
Through DSC commands, geographically distributed software developers can perform source code checkouts/checkins as if they were all working at the same site.
The DSC approach offers several advantages over other distributed source control mechanisms:
* DSC is easy to use and does not require software developers to make major changes to their work habits.
* Since DSC is compatible with CVS, users can mix DSC and CVS commands to suit their needs.
Other approaches often require that all operations be performed through a particular interface.
* Since DSC sits on top of CVS but does not modify it, DSC can easily grow with later versions of CVS.
Other distributed source control approaches typically require modifications to the underlying source control program.
Access to the DSC from the Top Layer will initially be controlled through a web-based forms interface.
DSC is currently in beta, and will be matured for integration into the primarily user interface as Java editors mature.
Tasks within this middle layer will primary consist of the adaptation of existing tools into the high performance computing environment.
The PSE Bottom Layer - Libraries
The bottom layer consists of portable software libraries which insulate the genome research group from the specifics of the underlying computing architecture.
Libraries will be incorporated for efficient message passing, I/O, multi-threading, algorithm design and distributed visualization.
Efficient Inter/Intra Message Passing.
The message passing interface (MPI) forum has recently setup a message passing standard that has been implemented by MPP vendors (Clarke et al. 1994, Dongarra et al. 1995).
This standard is for single MPP machines and not for a heterogeneous collection of MPPs.
Alternatively, MPICH (see MPICH reference) is based on MPI but will work on a collection of machines.
This version is not fully optimized for heterogeneous networks or for high performance ATM networks.
The MPI committee is currently working on a standard for a heterogeneous environment known as MPI2 (Walker).
To date, no standard has been developed and given the time for implementation by the vendors, coupled with the 3-year time frame of this proposed work, an alternative message passing library will be initially used.
The MSDE uses object-oriented methodologies making it possible to easily move to MPI2 when it becomes available.
The parallel virtual machine (PVM/PVMPI) message passing software library will be used to perform the communication between all the platforms comprising the virtual environment.
PVM (Geist et al. 1994, 1995, Beguelin et al. 1993) is currently the most efficient and robust message passing library for handling the communication between the machines in the virtual environment because PVM takes advantage of high speed interfaces such as ATM, as well as being able to operate within a heterogenous environment of networks and computers.
As importantly, PVM supports fault-tolerance through dynamic machine configuration of the virtual environment.
In addition, Dongarra and co-workers have compared their MPP PVM implementations with MPI on various MPPs including the Intel Paragon and IBM SP2 platforms.
They found essentially no latency and bandwidth differences between PVM and MPI on these platforms.
Currently, ORNL has an ongoing collaboration with Sandia National Laboratory to link together with ATM the two largest Paragon systems in the world.
The OC3 connection extends from ORNL through ESNET to SNL and should be completed by August.
The LSMS is one of the testbed codes and even for a small 256 atom run using a 13 atom local interaction zone that is split evenly between the 512 GP Paragon and 1024 node MP Paragon located in the CCS at ORNL requires the exchange of 6.8 megabyte message every 15 seconds, and obviously stresses the network.
The experience gained in this project is invaluable and provides us with the necessary expertise to design highly efficient heterogeneous applications.
We expect the six highly interleaved processes in the genome analysis engine to require extensive message passing.
Active messages will be used to take advantage of computer architectures that have multiple processors per node by threading an executable onto a second processor.
This executable subprogram, if it is the master node, will collect messages destined for the other machines in the virtual environment or it will send its messages to the master processor in a binary tree fashion.
The master processor will pack into separate buffers the messages for each machine in the virtual environment and send them to their respective destinations.
This approach reduces the latency cost by sending at most n-1 messages onto the network (where n is the number of machines making up the virtual environment), and depending on the message length, these may nearly use the full bandwidth of the network.
In addition, by overlapping the communication with computation, the overall cost of sending messages onto the network should result in at most a few percent overhead, making the virtual machine look like a single large MPP.
This algorithm will be developed into a software library which will be callable from C and FORTRAN, and will be made available to other projects.
Fault-Tolerance, Parallel Portable Check-Pointing and I/O.
The genome computations proposed require the development of heterogenous, fault-tolerant computer codes.
Fault-tolerance will be incorporated by leveraging CUMULVS middleware whose communication is also based on PVM. CUMULVS (Papadapoulos) provides fault-tolerance for distributed simulations through heterogenous task migration and user-directed check-pointing.
CUMULVS hides the I/O from the application by sending the data as a message to the CUMULVS daemon rather than interacting directly with the file system.
The primary advantages in this approach are that the performance of the application is increased because the cost of I/O is the cost of sending a message.
Second, in a heterogeneous environment, the difference in performance of parallel file systems can result in a load balancing problem.
In this approach, this difference is removed by shifting the I/O cost to another process, independent of the compute process.
Equally important is the fact that this approach effectively hides from the user all intrinsic details associated with using high performance file systems.
The check-pointing within CUMULVS is saved to local disk but does not use a parallel file system.
This is problematic in the case of a large data stream.
We propose to develop the interfaces that provide the connectivity to high performance file systems, including HPSS (see Appendix II), using CUMULVS data management.
This library will make no distinction in its handling of data between check-pointing and standard I/O.
Portable Multi-Threading Libraries.
A portable multi-threading library provides a coherent interface to the user that directly maps onto each platform's native library.
This library will greatly expedite the development of portable, heterogeneous genome analysis computer codes and will conceal the platform dependent multi-threaded software from the user.
Since the library simply maps its calling structure onto the machines multi-threaded calling structure, it provides a coherent interface, but does not address the larger problem of designing a multi-threaded code that runs optimally on a variety of shared and shared/distributed platforms.
This problem will be addressed in Task 2 - Portable Algorithm Design and Optimization for Genome Analysis.
Distributed Visualization.
An important aspect of this large collaboration will be the ability to simultaneously visualize processing data created during an analysis, or post-processing data retrieved from the data warehouse.
CUMULVS is a valuable tool for use in many large scientific applications because it allows scientists at different locations, using different network protocols and platforms, to visually monitor and remotely steer a large heterogeneously distributed application.
CUMULVS handles the details of collecting and sending distributed data fields to and receiving steering parameters from multiple dynamically attached viewers.
These viewers can be commercial packages or customized viewers for a specific application.
CUMULVS ensures that each viewer has a time-coherent view of the parallel data, and it ensures steering parameter coherency across multiple viewers.
We expect, for example, that processing of genomic sequence will have to be monitored simultaneously by several scientists responsible for the distributed analysis as well as at databases supplying the data (Genome Center databases) and receiving the analysis results (The Genome Sequence Database (NCGR) and the Genome Database (JHU)).
Data import visualization modules for CUMULVS have already been written for a stand-alone commercial package AVS (Papadapoulos and Kohl).
This is a different environment than web-based visual systems which have to communicate through a server.
Data communication to web-based viewers will require the development of Java classes and JavaScripts to receive and transmit data through the server using CUMULVS communication with analysis processes or with the data warehouse.
In addition, cgi scripts will be created to make visualization data available to both HTML and VRML pages.
Metadata within the data warehouse will be used to incorporate dynamic data linkages within HTML and VRML visualizations.
The following summarizes our goals for Task 1:
year 1
Develop base PSE working prototype
Portable multi-threaded library for paragon
Begin active messages on Paragon
Parallel I/O to paragon and T3E
JAVA primary user interface
Base editor
Simple scheduler
Base electronic notebook
CUMULVS cgi for 2D web page visualizations
year 2
Begin portable multi-threaded library for Cray J90, SGI, Convex
Begin active messages on Cray J90, SGI, Convex
Begin parallel I/O for IBM SP, Cray J90, SGI, Convex
Base CONDOR scheduler
Base conferencing tools
Extend electronic notebook
Integrate Distributed Source Control into JAVA editor
Begin JAVA-CUMULVS interface cgi for VRML visualization
year 3
Complete portable multi-threaded library for Cray J90, SGI, Convex
Complete active messages on Cray J90, SGI, Convex
Complete parallel I/O for IBM SP, Cray J90, SGI, Convex
CONDOR scheduler
Complete conferencing tools
Complete JAVA-CUMULVS interface
Dynamic hyperlinked VRML visualization
Task 2.
Portable Parallel Algorithm Design and Optimization for Genome Analysis
Scalability and Portability Issues
The development of algorithms that are portable and heterogeneous is a major issue for this project.
An algorithm designed on one platform is not necessarily the best algorithm for another, and it is quite possible that neither is optimal for a heterogeneous environment.
Architectural hardware and software differences and machine topology are major contributing factors to this problem.
Current and future machine architectures are incorporating some form of shared memory multiple processing and in addition, are configuring entire machines based on loosely coupled collections of shared memory machines.
Therefore, multi-threading is included as part of our design criteria.
To efficiently design portable, heterogeneous and highly optimized codes will require specific tuning of the components of the individual codes within the genome analysis engine.
This issue has been dealt with in numerous previous codes developed at ORNL.
For example, an implementation of a parallel fast Fourier transform (FFT) algorithm was relatively simple since entire vectors reside on a machine, and in the case of a heterogeneous environment are not split among machines.
In this situation all that was needed is a routine that maps a parallel FFT algorithm from one topology to another.
On the other hand, experiments on efficient memory accesses in a multithreaded environment proved more problematic because of machine hardware differences such as the size of second level memory, and the number of cache lines and bus bandwidths which all greatly affect the implementation details.
The design of optimal algorithms in the genome analysis domain will likewise require careful consideration of these factors.
An Overview of Processes within the Analysis Engine
The genome sequence analysis engine will require the integration of many different algorithmic codes into a seamless automated processor capable of achieving a high-overall rate of throughput.
The basic methods for the recognition and modeling of genes require the execution of several interdependent calculations, creating a number of different types of information.
Our genome algorithm library will consist of six distinct types of algorithms: pattern recognition, homology-based calculation, statistical measurement, gene modeling, sequence assembly state calculation and data mining.
These will need to be combined into a coordinated toolkit to synthesize the complete analysis.
Previous genome analysis systems have used these components in a linear order, and basically in a way which made these calculations independent of one another.
Because of this, significant potential for advantageous cooperative interaction between the component calculations has been lost.
Each component process can provide information of value to the others working on a particular segment of the DNA sequence, and only through a combined discovery process can the best analysis and gene modeling results be achieved.
We propose to take these components and develop an entirely new algorithm that is both a function and data decomposition paradigm which is sometimes referred to as a multiple instruction multiple data (MIMD) algorithm.
This type of approach would be quite challenging, requiring the coordination of all the components to ensure proper load balancing.
To obtain proper load balancing and optimal efficiency will require restructuring the current algorithms as well as possibly incorporating different levels of parallelism with the choice of a particular layer being based on the availability of computational resources.
This approach, though not new, is rather unique since most parallel applications and all current Grand Challenge applications are based on a data decomposition paradigm, better known as a single instruction multiple data (SIMD) algorithm.
This type of algorithm would truly demonstrate the full advantages of using MIMD architectures, by using MIMD algorithms rather than the current SIMD ones.
Unlike previous genome analysis systems, the implementation of a function and data decomposition model for the analysis steps, where information from various components being run on a particular segment (contig) of DNA sequence provide information to each other, results in the overall best models for genes and other important features.
In addition, implementing this type of paradigm is even more important because much of the data to be analyzed is only partially sequenced and not fully assembled.
The basic nature of these types of sequence is illustrated schematically in Figure 4.
[gen-4.gif] Basically in a large genomic clone (perhaps 30000 to 150000 DNA bases long), small overlapping fragments of the sequence generated randomly are assembled into contigs, whose overall order and position is not initially known and which have gaps between them.
Many segments of the sequence data, unknown at the outset, will have emerging importance to one another as the analysis proceeds, and these relationships need to be discovered and exploited within the overall analysis process.
The presence and structure of genes and other signals in the sequence, and the structure of the sequence assembly itself, are being determined simultaneously.
We believe that this and other important inter-relationships in the analysis provide an inherent problem structure which works well with the distributed heterogeneous model for the computing we propose to use, and we will therefore place a strong emphasis on message passing efficiency.
With implementation of this analysis model, and given significantly greater computing power to accomplish it, a new level of comprehensiveness and accuracy in genome analysis can be obtained which was not previously possible.
The analysis engine will consider the state of the assembly, the available homology and data mining information, and the predictions of pattern recognition and statistical systems concurrently, within this integrated process.
This will involve using the set of contigs of a large genomic clone and the attached metadata as the basic currency of the analysis and the system will analyze a set of contigs as a single informational unit and simultaneously process the multiple sequences from a given clone-based contig set.
This is illustrated schematically in Figure 5.
[gen-5.gif] The system will input any existing information about the order of these contigs, any characterization of gaps between them, and other physical map-related metadata.
Available information about order in the assembly will be used by the system to consider genes which span gaps.
Homology information and data mining information will be used in an integral way to detect and model genes, identify genes which cross gaps, infer additional order between contigs based on database information, and help determine what parts of genes are missing in gaps or off the end of a contig or clone.
Gene model information will be used to help collapse related EST assemblies which in many cases cover separate parts of the same gene.
The identification of genes and their extent can be used to decide which gaps are important to close with high quality sequence at the experimental genome centers.
Sequence quality or redundancy information from the assembly (if available) can be used to localize regions where sequence quality is low and indel detection system may be useful to apply.
The majority of this information would not be discovered using the standard approach.
We summarize our analysis goals as follows:
(i) Comprehensively analyze and model genes at all levels of sequence finishing and assembly;
(ii) Automatically recognize and use types of analysis appropriate at different levels of assembly and in different parts of the sequence at a given time;
(iii) Use all available information such as protein homology, EST calculations and data mining results in an integral way to both recognize and model the genes;
(iv) Use automated pattern recognition and homology-based procedures to determine the extent of individual genes, even across multiple sequence contigs in partial assembled sequence, recognize when parts of genes are missing where possible, and model partial and multiple genes accurately;
(v) Utilize sequence quality information to minimize the effects this may have on feature recognition by selectively using more computationally intensive frame-tolerant algorithms and statistical frameshift detection.
Overall Gene Modeling Methods
The goal of gene modeling in the analysis engine is to combine coding exons (exons are segments containing the gene's message), noncoding exons, and many other features detected using statistical measures, pattern recognition, and similarity data into a comprehensive description of the genes present in the sequence.
Algorithms to construct such models use several methods such as dynamic programming (Gelfand and Roytberg 1993; Synder and Stormo 1993; Xu et al. 1994b, 1996b (reprints in Appendix III)), generalized hidden Markov Models (Kulp et al. 1996), or applications of graph theory (Gelfand et al. 1996a, 1996b).
ORNL, LBNL and ANL are world leaders in the development of such methods.
Within the analysis engine, six basic types of algorithms in the genome toolkit will be linked to locate and construct gene models.
We are currently expecting to implement three overall gene recognition and modeling methods:
(1) The ORNL Gene Modeler
is an outgrowth of the ORNL GRAIL Internet sequence analysis servers.
GRAIL is a modular system which supports the recognition of gene features and gene modeling for the analysis and characterization of DNA sequences (reprint in Appendix III).
GRAIL uses multiple hybrid statistical and neural network-based pattern recognizers and a dynamic programming approach to constructing gene models (see Appendix II).
GRAIL recognizes protein coding regions (exons), poly-A addition sites, potential promoters, CpG islands and repetitive DNA elements.
XGRAIL also has a direct link to the genQuest server allowing characterization of newly obtained sequences by homology based methods through accessing a number of databases using a number of comparison methods including: FASTA, BLAST and a parallel implementation of the Smith-Waterman algorithm (described later) which utilizes the DEC cluster at ORNL CSMD.
Following an analysis session the user can use an annotation tool to generate a ``feature table'' describing the current sequence and it properties.
All of this information is presented to the user in graphic form in the X-window based client-server system XGRAIL, and through Web based interfaces.
Since its initial development in 1991 (Uberbacher and Mural, 1991), the GRAIL system at ORNL has become the world standard for predicting protein coding regions (exons) and modeling genes in DNA sequences.
The GRAIL system currently analyzes about 17 million bases of DNA sequence per month (using methods which are simpler and less comprehensive than what is proposed in this GC project).
Combined, GRAIL and genQuest tax the computational power of a 16 DEC-Alpha workstation cluster maintained to support these servers.
The experience with GRAIL at ORNL gives us an understanding of the needs of the genomic / biomedical research community.
[gen-6.gif] Figure 6 illustrates a number of biologically relevant features which GRAIL attempts to recognize in a DNA sequence, which are presented to a user in the XGRAIL interface.
The central grey bar reflects the local concentration of G and C nucleotides which, in human DNA, correlate with the density of genes (the particular region represented in this figure has a higher percentage of G+C and is gene rich).
Two other features are superimposed on the central grey bar: The magenta boxes mark the locations of ``CpG'' islands (Gardiner-Garden and Frommer 1987), regions with a locally high concentration of the dinucleotide CG, which are thought to be associated with genes and control of gene expression.
The yellow boxes indicate the presence of various DNA elements which are highly repeated in the human genome and are generally considered ``junk'' DNA (but may have unknown function).
The peaks above and below the central grey bar are regions of the sequence which the program predicts as protein encoding regions (exons) which are presumably parts of genes.
Genes can be found on either strand of a DNA helix which is why there are peaks both above and below the central bar.
The colored bars immediately above the various peaks give an indication of the confidence level of the predictions for each exon.
Green bars represent very likely exon candidates (
In the central region of the figure there are a series of cyan boxes linked together by yellow lines.
These represent a group of exons which the analysis predicts belong to the same gene.
The concatenation of the sequence represented by these cyan boxes forms a gene model, that is, the portion of the DNA sequence which encodes the protein product of the gene.
None of these features are obvious by casual inspection of the DNA sequence, but rather are found by examining the sequence with the various algorithms that make up the GRAIL system.
The ORNL Gene Modeling method (Xu et al. 1996c / reprint in Appendix III) we will use for this project is a new development and is considerably more complex and sophisticated than the on-line GRAIL system, which has limited accuracy and throughput capabilities.
This gene modeling method is capable of using protein homologs and ESTs to refine gene structure found by pattern recognition methods.
The problem is approached as an optimization based on features detected and weighted constraints, and is solved using a dynamic programming algorithm.
The basic algorithm scans through possible gene coding regions from left to right and constructs an optimal gene structure over the sublist which is consistent with a given reference set of constraints from homology calculations.
The algorithm runs in O(N2K2) time and O(NK) space, where K is the maximum number of reference models for an exon candidate and N is the number of possible exon candidates.
Unfortunately the complexity is such that the compute time is very long and as part of this project we will develop new approaches to reducing this complexity in order to achieve better algorithmic scaling.
(2) Generalized Hidden Markov Models for Gene Model Construction (LBNL/Haussler)
This method involves a combination of neural network predictions with ``generalized'' hidden Markov model parameters in a complex gene parser.
Hidden Markov Models (HMMs) for DNA can be viewed as generative stochastic models of sequences that go though a sequence of hidden states, and in each hidden state they generate a single letter in the alphabet {A,C,G,T} according to certain probabilities associated with that state (Krogh et al. 1994a, 1994b, 1996 / reprint in Appendix III).
The sequence of hidden states forms a Markov chain, in the sense that which state generates the nth letter depends only on which state generated the n-1 letter.
This contrasts with (non-hidden) Markov models of sequences, in which the sequence of letters itself forms a Markov chain, i.e. the nth letter depends only on the n-1 letter, or on the letters at positions n-k,... n-1 in the case of a kth order Markov model.
In a generalized HMM, each state can generate a string of one or more letters according to a probability distribution specified by some arbitrary mechanism, particular to that state.
The only thing demanded of this mechanism is that the probability can be efficiently calculated (i.e. likelihood) of any string under the distribution defined by the mechanism.
Models of this type were described in Stormo and Haussler (1994), for the case of only two possible states, Exons and Introns, and the Markov chain for the states is trivial, in the sense that it alternates back and forth between these two states with probability 1.
Each time such a model is in the exon state, it generates a string according to the probability distribution associated with exon strings, and each time it is in the intron state it generates a string using the intron distribution.
The output is the concatenation of these generated strings.
To use such a generative model for recognition of introns and exons in a DNA sequence, a dynamic programming method can be used to ``invert the generative process'' and find the most likely sequence of individual strings and their states that were concatenated to make the sequence.
This is referred to as ``parsing'' the sequence (constructing the best gene model).
Recently, this model has been extended to include more than 2 states, with general Markov transition probabilities between states.
This is the basis of ``generalized'' HMMs.
Another advantage to the flexibility of generalized HMMs is that it is easy to add other capabilities to the gene modeling system, such as the ability to incorporate homology information discovered by searches of a protein database, and to find promoters, RNA genes, known repetitive DNA sequences etc., as soon as states for these have been defined with appropriate probability distributions.
Once these states are defined one can ``wire them in'' to the existing HMM architecture by modifying the Markov chain transition probabilities to include the new states.
(3) Pevzner's Homology-Based Method
(collaboratory participant Pevzner at USC) (Gelfand et al. 1996a, 1996b) is a homology-based gene modeling algorithm which combines multiple similarities to protein homologs and ESTs to construct a gene model.
Sequence level signals (like potential splice sites) are recognized statistically.
This method uses a new combinatorial approach to the gene modeling, which uses related proteins to derive the correct exon-intron structure.
Instead of employing statistical properties of exons, the method attempts to solve the combinatorial challenge of finding a set of blocks in a genomic sequence whose concatenation (splicing) fits one of a known protein.
The approach is based on the following idea.
Given a genomic sequence, it first finds a set of potential blocks which contain all true exons.
This can be done by selecting all blocks between acceptor and donor sites (statistically recognized edge signals) with further filtering of this set (in a way that does not lose the true exons).
The resulting set of blocks, of course, can contain many false exons and currently it is impossible to distinguish all ``true'' exons from this set by a statistical procedure.
Instead, all possible block assemblies are explored to find an assembly with the highest similarity score to each given similar known target protein.
The number of different block assemblies is, of course, enormous, but the spliced alignment algorithm, which is the key ingredient in this method, scans all of these possibilities in polynomial time.
In this algorithm, the exon assembly problem is reduced to the search for the optimal path in a graph.
Vertices in this graph correspond to the blocks, arcs correspond to potential transitions between blocks, and the path weight is defined as the weight of the optimal alignment between the concatenated blocks of this path and the target sequence.
The three gene modeling methods just described have different structures, use the basic algorithmic components differently, but in many basic respects share common characteristics and needs for underlying statistical, pattern recognition, and homology calculation algorithms.
Therefore, we will build a genome analysis library based on an object-oriented philosophy enabling the gene modeling codes to use common components and to easily implement new features.
Each overall algorithm, depending on the course of the analysis, requires the use of a large and very variable number of component steps (calls to other algorithms such as pattern recognition or homology calculations).
The algorithmic complexity of each of the overall modeling processes varies depending on what is found in each sequence, but in general, all scale very poorly.
For example, the ORNL Gene Modeler scales as N4, while most of the components parts scale somewhat better, between N3 and N.
The overall modeling algorithms are very recent developments, and their complexity can potentially be reduced significantly through further investigation.
As part of this project we will develop new approaches to these algorithms, reducing their complexity, which will result in better algorithmic scaling.
Common to all gene modelers are methods for homology calculation from protein sequence and EST databases and the use of neural networks and statistical methods for the recognition of a host of sequence signals.
The homology calculations involve well understood algorithms which are highly parallelizable, but lacking in computational performance.
The three gene modeling methods may use more sensitive and computationally costly algorithms than are actually needed.
The evaluation of more efficient algorithms, with possibly more inherent parallelism for particular component steps, will be addressed as part of this Grand Challenge project.
Furthermore, additional investigation is needed to modify the general purpose algorithms (described in the next section) for their specific function within the gene modeling context.
These algorithmic enhancements to the components in the genome analysis library will have substantial benefit to the throughput as well as the overall goals of the project.
Example Processes in a Gene Modeler
As an example for this overall process we describe the basic steps for the ORNL Gene Modeling method.
First, the initial components of a gene are recognized with an initial sweep of the sequence contigs by pattern recognition systems and statistical measures.
As soon as results begin to be returned for a given contig, additional homology-based analysis begins based on what is initially recognized and continues until all recognized initial portions of genes in the contig have been tested against EST and protein databases.
While this is being done, additional segments of the sequence may enter the pattern recognition and statistical measurement stage of analysis to maintain a continuous flow.
Initially, the development of a function and data decomposition algorithm will be performed on the pattern recognition and statistical measurement systems.
A primary objective of this initial development is to ensure the completion of all relevant analysis parts occurs at approximately the same time for a given sequence segment (contig).
This approach will effectively hide the latency and I/O costs to the warehouse.
However, homology-based processes will be held up while parts of the earlier analysis remain incomplete.
Certain statistical calculations such as the localization of repetitive DNAs are quite slow compared to faster parts of the pattern recognition system.
This will require either additional parallelism or additional processor power.
Furthermore, several homology-based calculation methods are also considerably slower than the pattern recognition and statistical measures, and again will require the same computational approach as the above.
In addition, several algorithms for homology estimation (described later) are quite different from one another in computational performance and must be properly balanced to ensure synchronicity.
The gene modeling program uses information calculated by the initial pattern recognition and statistical measurements and requests a large number of additional homology-based tasks, and also exchanges data with the sequence assembly state calculator.
The sequence assembly inferencing system will provide information about the state of the assembly relevant to the choice of gene model construction methods for a given situation.
Likewise, the success of a gene model's construction provides input to the state of the sequence assembly.
Related to this is a need for the gene modeling algorithm to resolve inconsistencies such as identifying previously missing parts of genes inferred from homology data (perhaps due to incomplete sequence assembly or pattern recognition failure).
An even later process will involve ``brute force'' sequence comparison for entire regions in the sequence for which genes have not been identified up to this point (where pattern recognition may have failed).
As gene models are constructed, data mining at remote databases for information related to these genes can commence.
Results from the data mining processes can further direct the analysis events within the engine and invoke additional pattern recognition tasks (for example search for additional gene promoter signals based on the gene class identified by data mining).
Interleaving these many somewhat unpredictable processes is a basic requirement for throughput and places complex constraints on the design of a function and data decomposition algorithm, particularly on the message passing and synchronization of the elements comprising the system.
We summarize this overall process (for the ORNL method) in the following somewhat overly linearized and simplified steps:
(i) Sequences related to a genomic clone enter the gene modeling algorithm from the warehouse.
(ii) Initial statistical tests and pattern recognition are done on one or several contigs from the clone.
(iii) Results from step (ii) direct several homology-based calculations against several databases for each exon in the contigs.
(iv) Assembly state inferencing system is used to detect sequence contigs which are related through components of shared genes.
This information is used to select gene model construction constraints and options for each contig or for genes spanning multiple contigs.
(v) Optimal gene models are calculated for single or multiple genes and inferred partial genes.
Gene models are used to update estimates of the sequence assembly state.
(vi) Remaining portions of the sequence without genes are brute force searched using homology methods.
Additional genes may be identified and subject to the previous gene modeling steps (iv 
Alternatively Pevzner's method can be used for these additional genes.
(vii) Data mining is facilitated to discover information in remote sources of potential relevance to the newly discovered gene.
Individual data mining steps may take considerable time.
Data mining results may require additional analysis (particularly pattern recognition) steps and links to relevant remote data are retained for later visualization and storage.
Basically, six interleaved and very asynchronous processes, which strongly depend on one another (pattern recognition, statistical measure, feature selected homology calculation, assembly inferencing, gene modeling, and data mining) must be dynamically managed to run with similar overall throughput.
The amount of time required for each step and for each segment of the sequence will be highly variable and dependent on what is found in previous and sometimes later processes.
It is inevitable that this situation will rarely be ideal and the algorithmic design must encompass a hierarchical parallel strategy that is flexible and can dynamically switch to another level of parallelism as more computational resources become available, while tracking individual analysis components and data, so that the whole process runs efficiently enough to process between 2 and 20 megabases of sequence per day.
The complexity and interdependencies of the ORNL gene modeling algorithm are similar in character to those in Pevzner's Method and the LBNL/Haussler HMM method.
However, even though some similar base components are used in the engine, the chain of events and the specific dependencies between processes are very different.
While we currently expect that each of these basic methods will be run independently on each DNA sequence contig or clone, dependencies between the three methods (for example as in step (vi) above) may actually be rather useful and may lead eventually to further overall gene modeling algorithm refinement.
Two additional gene modeling algorithms GeneParser (Snyder and Stormo 1993) and GeneID (Guigo et al. 1992) may be additionally considered depending on the course of their development.
Gene Modeling in Partially Assembled Sequence
One of the added complexities which has not been fully explored is modification of these algorithms to deal with the incomplete nature of the target genomic sequence.
Since in many cases, multiple contigs (sequence fragments) will be present which have gaps between them and indeterminate ordering, genes may only be partially present with parts missing in these gaps.
Furthermore parts of a gene may exist on several fragments, although this may be unknown to the original sequencing laboratory.
Several of these situations are illustrated in Figure 5.
The gene modeling algorithms must be modified to compensate for the partial or gapped nature of the gene models since these phenomena violate constraints used in current gene modeling algorithms.
Our preliminary experiments indicate that these modifications can be accomplished without increasing the computational complexity of the algorithms.
However, the analysis engine must know when a gene model is partially missing and where, in order to apply the proper algorithm in the proper way.
This is discussed below in section (d).
An expert system to make these types of determinations is under construction at ORNL and could also potentially be used by all gene modeling methods.
Automated Multiple Gene Modeling.
Extension of the gene modeling algorithms described above are being made to process multiple genes in an automated fashion (Xu et al. 1996b (reprint in Appendix III)).
Currently, although single genes can be modeled semi-automatically, analysis of longer sequences require significant manual intervention and trial and error model building (for example in GRAIL).
Changes in the objective function used by the gene modeling algorithms and additional bookkeeping are necessary to accomplish multiple gene modeling in an automated mode, and fortunately the complexity only increases by N, where N is the number of genes present.
Algorithmic Components
In the following sections (a)-(d), we describe in brief detail the individual algorithmic components which contribute to the gene modeling process: (a) pattern recognition, (b) homology calculation, (c) statistical measurement, and (d) assembly state estimation.
The remaining Analysis Engine component, data mining, is discussed in Task 3.
(a) Pattern Recognition Algorithms
A variety of pattern recognition systems will be included in the engine, including statistical and neural network systems for coding sequence recognition, translation start recognition, splice site recognition, promoter signal recognition, and exon candidate scoring.
Many of these, such as the ORNL exon scoring system have a hybrid structure which involves the calculation of a set of individual statistical measures for each sequence region, followed by the fusion of the results using a neural network for final prediction or feature scoring (Uberbacher and Mural 1991, Xu et al. 1994a, 1996b, 1996e, Matis et al. 1996).
Most of the computing time is spent in the statistical tests and some of these tests are much more computationally intensive than others.
One example is the calculation of a multitude of sums for a number of Markov chain models.
Again the potential advantages of a scalable parallel solution is significant.
Some of the neural network systems, such as the ORNL exon candidate network, use the outputs of previous neural networks for other features such as splice signals.
Similar methods are used in the LBNL/Haussler HMM and Pevzner systems for feature detection, and in some cases (for example, promoter detection), time-delay neural network systems (Kulp et al. 1996) are used.
Based on the performance of the GRAIL gene recognition Internet server (see Appendix II), we estimate that approximately 640 Paragon processor hours (or the equivalent) will be needed each day, given current pattern recognition systems for the combined ORNL, LBNL/Haussler HMM and Pevzner methods.
(b) Homology-Based Calculations
Calculation of similarity due to homology (``homology-based calculation'') between newly obtained sequence and archives of various types of information is a complex problem with many different facets.
A range of algorithms is available or under development to solve somewhat different homology calculation problems.
Each available method has particular strengths and specialized uses.
The range of efficiency is not adequately captured by examination of the limited range of algorithmic complexity (N2 for Smith-Waterman type dynamic programming algorithms (Smith and Waterman 1981, Gotoh 1982), to between N and N2 for the fastest, least sensitive algorithms (Altschul et al. 1990, Pearson and Lipman 1988)).
Development in this area is rapid and in many ways the use of the general purpose homology calculation algorithms within the overall gene modeling processes is new.
A full range of algorithms needs to be used for various tasks, and our intent is to use the simplest algorithms whenever possible and wherever they are adequate, and use the more computationally intensive algorithms more selectively, only as needed.
One of the research challenges of the project is to evaluate, improve and optimize the use of these algorithms for several of the new applications.
We hope to be able to customize algorithms for our needs, and evaluate the adequacy of simpler algorithms for some tasks.
Even using time consuming algorithms selectively, the computational requirements of this portion of the project will be very substantial.
We estimate for the expected megabases of sequence per day, that this type of calculation in the context of the gene modeling systems will require the equivalent of 4000 Paragon processor hours per day unless further simplification of algorithms and methods is accomplished.
To meet the goals of the project will require the development of new efficient computational techniques and algorithms that are both scalable and encompass a hierarchical parallel structure resulting in significantly improved scaling properties and performance.
A very brief overview of some of the general purpose homology calculation algorithms for this problem is provided below:
Dynamic Programming / Smith-Waterman.
The Smith-Waterman (SW) algorithm (Smith and Waterman 1981, Gotoh 1982) is a dynamic programming algorithm, which can be conveniently described in a grid-like graph.
One sequence is placed vertically to the left of the graph, and the second sequence is placed on top of the graph.
Each cell (i,j) of this matrix contains a score based on the similarity of the ith element in the horizontal sequence to the jth element in the vertical sequence and the scores of the diagonally preceding cells.
Diagonal edges represent matches or substitutions, and the vertical and horizontal edges represent insertions and deletions.
The algorithm starts at the upper left corner, and tries to find a lowest scoring path between the upper left corner and the lower right corner (which corresponds to an optimal global alignment of the two sequences).
With proper initialization, local alignments can be found in a similar way.
The complexity of the SW algorithm is O(mn), where m and n are the lengths of the two sequences compared.
There have been several attempts to parallelize the SW algorithm itself, but these algorithms have not been efficient in terms of their processor utilization.
Data parallel approach seems more natural for parallelizing the use of SW for the search of large databases or long sequence regions.
Global comparisons of cDNAs or EST assemblies against long genomic regions, or long DNA-DNA comparisons often use this algorithm.
Also comparing long sequence regions against each other requires significant space.
For example, 1 Mb x 1 Mb requires 1012 matrix elements.
Special algorithms have been designed to dramatically reduce space requirements (see Guan and Uberbacher reprint in Appendix III).
FASTA.
FASTA (Pearson and Lipman 1988) is one of the many heuristic algorithms proposed to speed up the database search process.
The basic idea is to add a fast prescreen step to locate the highly matching segments between two sequences, and then extend these matching segments to local alignments using more rigorous algorithms such as Smith-Waterman.
FASTA starts its prescreen process by building a look-up table from one sequence in linear time, and then scans the second sequence to identify the matching segments by recognizing segments of bases having the same offset.
The prescreen step reduces the search space to a small portion of the database and thus decreases the search time significantly.
Depending on how the matching segments are extended to local alignments, the complexity of the algorithm is between O(n) and O(mn), assuming n 
FASTA has reduced sensitivity compared to Smith-Waterman and can therefore miss important but weak similarities.
In the ORNL Gene Modeler, FASTA is used however, because it appears usually adequate for EST and protein alignments assuming significant similarity.
It may not however be adequate when the method is extended to the use of weak protein homologs.
The use of weak homologs in gene modeling is not as yet a well evaluated methodology.
BLAST.
This heuristic search algorithm is based on mathematical results that allow the statistical significance of matching sequence segments to be estimated under an appropriate random sequence model.
In BLAST (Altschul et al. 1990), a maximal segment pair (MSP) is the highest scoring pair of identical length segments chosen from two sequences.
Recent results allow the estimate of the highest MSP score at which chance similarities are likely to appear.
BLAST minimizes the search time by concentrating on only those sequence regions whose similarity with the query sequence exceeds the chance similarity score.
The BLAST algorithm consists of three steps: compiling a list of highly-scoring words from the query, scanning the database for hits (Maximal Segment pairs - MSPs), and extending hits to local alignments.
The complexity of the BLAST algorithm is O(aw+bn+cnw/20w), where w is the number of words generated, n is the database size, and a, b, c are constants.
BLAST is faster than FASTA, but it doesn't allow gaps in its alignments, and is less sensitive.
BLAST is currently viewed as inadequate for homology calculation against protein homologs in the ORNL Gene Modeler, although its use, or use of a modified version, may be adequate for EST alignments.
A new version of BLAST called BLAST2 may contain increased sensitivity and will be implemented as a server at ORNL in collaboration with its author Warren Gish of the Washington University Genome Center, St. Louis.
It will be evaluated or perhaps modified to meet the needs of the project.
Even using BLAST as the homology calculator for both protein and EST searches, the ORNL gene modeler still retains 
Frameshift-Tolerant Dynamic Programming Sequence Alignment Algorithm.
As experimental data, DNA sequences are subject to error, and frameshift errors usually weaken homology recognition between sequences.
ORNL (Guan and Uberbacher 1995) has developed a new algorithm to find the optimal alignment in the presence of sequence errors (reprint in Appendix III).
This algorithm compares a DNA sequence translated in three frames with a protein sequence.
In the standard Smith-Waterman algorithm, a score matrix cell (i,j)'s value can depend on three other matrix cells: (i-1,j-1), (i,j-1) and (i-1,j).
In the new alignment algorithm we consider not only the three cells in the same matrix, but also the same three cells in the other two frames' matrices.
That is, when computing the alignment for a given frame of translation, we also consider whether there is a better partial alignment in either of the other frames prior to this point that can be continued by shifting the frame to the one under consideration.
The algorithm's complexity is O(mn), but several times slower than Smith-Waterman.
Searching all sequence which enters the engine with this algorithm has significant utility for detecting errors in the sequence (important to genome centers) and locating potential pseudogenes (genes which were disrupted by evolutionary accidents).
We have implemented several of these algorithms on the Intel Paragon with near linear speed-up.
From the parallel algorithm design point of view, most of our algorithms can be viewed as data parallel algorithms, that is, the same program is run on each node but on a different part of the database.
A comparison of the basic speed of these algorithms on a single processor is shown in Table I.
CAPTION: Table I: Comparison of BLAST, FASTA, Smith - Waterman and Frameshift Tolerant algorithms; alignment of a small (6500bp) DNA segment against the 20 megabase Swissprot protein database using a SPARCstation 20.
Frame Tolerant 5915 minutes SW (1-frame) 1788 minutes FASTA (1-frame) 82 minutes BLAST (1-frame) 28 minutes
Homology Calculation with ESTs.
ESTs are experimentally obtained sequences gene message fragments.
There are extensive databases of both public and private EST datasets.
EST homology estimation represents a special application for which the adequacy of algorithms has not been sufficiently tested.
This subproblem contains very significant scalability issues which need to be addressed if high-throughput processing is to be possible.
Examining the relationship between 20 million bases of DNA and 1 million ESTs (average length of 500 bases each) typically involves calculation of a score matrix of 1016 elements.
This single calculation using the standard FASTA algorithm requires a 60 hour calculation at a sustained rate of 40 gigaflops.
In comparison, comparing potential coding exon candidates in a 20 million base DNA sequence against the Genpept protein sequence database using FASTA would typically require about 10 hours at 40 gigaflops.
As was seen in Table I, more sensitive algorithms for this, which also consider possible sequence errors (important since this data is often of low quality), would require many times more computing.
As a result, we will evaluate several different types of algorithms, some with significantly greater complexity than FASTA, to evaluate related EST and protein information.
The number of data resources that must be searched is also large and the amount of data in them is increasing exponentially.
Each EST and protein resource will have to be searched every day, and it has been suggested that previously analyzed sequences should be reanalyzed again and again (perhaps weekly) as new data enters the protein and EST databases.
ESTs are only one facet of the computation needed in the engine outlined in Figure 2.
Without retesting data, we estimate the daily need for the EST calculations at 2500 paragon processor hours for the combined ORNL and LBNL/Haussler HMM methods.
The Pevzner method does not currently use ESTs but is likely to in the near future.
Similar demand can be expected from this method.
The potential of the three methods to share EST and protein homology searches will be investigated as a way of saving computational cost.
Each method requires the search of different specific sequence segments determined by it, although these are bound to have some commonality which can be exploited.
We also expect that specific modifications of the more efficient algorithms may make them more useful for this application.
Detection of Protein Homologs with FASTA and Dynamic Programming.
While EST resources contain gene message fragments for the human genes we are attempting to find and characterize, protein homologs represent the products of these genes in human and related genes in other organisms such as the mouse, drosophila, yeast, etc.
Unlike the case of ESTs where the similarity between an EST and the genomic sequence should be very high in the coding exon regions, protein homologs may be relatively dissimilar at the sequence level.
This also implies the use of a more sensitive and costly algorithm to produce these alignments such as FASTA or even Smith-Waterman (dynamic programming with complexity NxM).
We estimate a daily requirement of the equivalent of 1500 Paragon processor hours for protein homology calculations (using FASTA).
(c) Statistical Sequence Measurements
Large-Spacial-Scale Statistical Analysis of Genome Properties.
Important information can be obtained through statistical measures obtained from the sequence, such as determination of statistical isochore or stochastic parameters which describe the long-range characteristics of the sequence domain.
As long contiguous regions of the genome are sequenced, analysis at a different scale and using different types of statistical methods becomes possible.
We know for example that genes are often grouped in clusters and families.
Many different genes are often grouped together and controlled by a common group of regulatory elements.
Identifying the position and significance of these control regions is challenging and computationally modeling the function of such complex control systems will become increasingly important for understanding large-scale genomic function.
Large-scale genomic regions also have interesting statistical properties related to their DNA base composition such as fractal and Markov properties.
These properties are poorly understood and it is likely that many new long-range statistical properties will be discovered which reflect functional properties of DNA structural and functional domains.
For example, patterns related to large-scale packaging of DNA in chromosomes are likely to be discovered.
This will become an increasingly challenging realm for the application of statistical data-mining methods.
Calculation of Isochore Parameters.
Much work has been done over the last several years to create formalisms to describe the compositional properties of DNA domains and propose stochastic models for their creation.
The nature of a given domain also has implications for the types and numbers of genes which are present in the domain as well as many components of gene regulatory processes (such as what types of regulatory elements are likely to be present).
Information from statistical isochore measurements has direct implications for later processes in the analysis engine which have to do with detection of regulatory features and hypothesize about mechanisms.
This active area of research will receive a flood of new data in the next phase of sequencing.
Long Range Statistical Correlations and Fractal Sequence Properties.
Participating researchers at LBNL are undertaking a systematic study of long-range statistical correlations in genomic DNA sequence as well as an examination of fractal properties.
Work at ORNL has involved the measurement of Markov chain properties of various DNA domains (Xu et al. 1994a, 1994d) and the elucidation of statistical and periodic patterns by Fourier transform methods related to chromosomal packaging and nucleosome formation (Uberbacher et al. 1988a, 1988b, 1989).
Genome Domain Replication and Duplication Dynamics.
A whole series of dynamic events occur in the evolution of DNA genomic sequence regions, such as large-scale duplication events, which produce multiple copies of genes or errors in the replication of DNA which produce DNA errors in genes and non-coding sequences.
Discovering evolutionary relationships between organisms and modeling these very dynamic processes is an important area of research which has significant impact on human genetic diseases.
LBNL (in collaboration with Denise Wolf - Berkeley Campus) has an ongoing effort to develop computational methods to model the dynamic behavior of such processes including the modeling of gene regulation and operons.
Data provided by this Grand Challenge analysis will provide significantly more information for these modeling processes.
Detection of Sequencing Errors.
Dynamic programming based algorithms for detecting errors in DNA sequence data based on statistical measures have been developed at ORNL (Xu et al. 1996a, 1996d) and will be facilitated as part of the statistical measurement task.
In particular, coding region frameshift information is critical when attempting to recognize and model genes in the sequence.
A paper describing this method in detail is contained in Appendix III.
This type of information is particularly import for genome centers who will wish to go back and examine the raw data underlying potential errors and correct these.
In some cases such apparent errors in frame are actually present in the sequence and represent an important loss of function of a gene through evolution (a pseudogene).
Repetitive DNAs.
Human DNA contains many interesting complex and simple perfect and imperfect repeating motifs.
Locating and masking such repeats is critical before homology tasks can be carried out in the analysis engine.
Otherwise the similarity of repeats between query and target sequences dominates homology calculations, obscuring other types of useful information.
Methods for identifying such repeats involve statistical comparison against a complex repeat reference library (Jurka et al. 1992) as well as fast methods for identifying arbitrary simple repeats.
A very fast method has been designed at ORNL and will be used in the analysis engine (Guan and Uberbacher 1996).
It is described in detail in a reprint in Appendix III.
(d) The Sequence Assembly Calculation
The assembly of sequence contigs into a completed contiguous sequence for a clone is one of the most difficult calculations in genomics and ``closing'' the gaps in the sequence is the most time consuming part of experimental sequencing.
Improvements in the basic technology associated with sequence assembly algorithms could have a dramatic effect on the cost of genome sequencing.
Errors and uncertainties in the sequence, as well as many repeated sequence patterns make it very difficult to reliably determine the overlap of individual sequence contigs.
Current algorithms which attempt assembly without human intervention have only mixed success and in most cases require significant intervention by a human expert to arrive at the correct sequence assembly result.
This type of process does not scale well for the high throughput stage of genome sequencing.
Identification of genes in the sequence and determining that the same gene is common to more than one contig provides independent information which can be used to link portions of a sequence assembly together.
The most widely used assembly algorithm, Phil Green's PHRAP (personal communication) could potentially be modified to add additional constraints imposed by knowledge of the extent of individual genes which will result from the daily analysis of partially assembled sequence in this project.
The Ace.mbly package (personal communication - J. Thierry-Mieg) has just recently been modified to allow the use of these types of constraints in the assembly of sequence.
Our intent is to collaborate with the developers of assembly algorithms so that the assembly state information obtained through our analysis can be used expeditiously at experimental sequencing centers.
Total Processing Estimate for Central Analysis Engine
Table II estimates computational costs in units of Paragon processor hours for central analysis tasks in the ORNL Gene Modeler, the LBNL/Haussler HMM method, and Pevzner's method for 20 megabases of sequence per day:
CAPTION: Table II: Daily Component Computing Estimates (Paragon Processor hours per day)
ORNL Gene Modeler Pattern recognition 320 Statistical tests 60 Homology calculations 1400 Gene modeling (main program) 200 Assembly state calculation 40 LBNL/Haussler HMM Pattern recognition 200 Statistical tests 120 Homology calculation 1000 Gene modeling (main program) 320 Assembly state calculation 40 Pevzner's Method Pattern recognition 20 Statistical measures 80 Homology calculation 1600 Gene modeling (main program) 480 Assembly state calculation 40 Total Paragon processor hours/day 6120
The following summarizes our goals for Task 2:
year 1
Portable codes for EST, protein homology
Portable codes for pattern recognition algorithms
Portable codes for ORNL gene modeling algorithm
Portable codes for LBNL/Haussler HMM gene modeling system
Portable codes for simple and complex repetitive DNA finder
Build sequence assembly inferencing algorithm
Portable code for sequence error detection algorithm
Sequence assembly constraint generation algorithm
year 2
Develop less algorithmically complex ORNL gene Modeling algorithm
Evaluate less complex homology search methods within ORNL gene modeler
Develop less algorithmically complex HMM gene modeling system
Reduce homology calculation requirements for Pevzner's gene modeling algorithm
Portable code for frame-tolerant sequence alignment algorithm
Portable codes for partial, gapped, and automated multiple gene modeling
Implement Sequence Assembly constraints in Ace.mbly and PHRAP
year 3
Implement parallel isochore and fractal DNA algorithms
Code for collapsing EST assemblies
Implement codes for multiple sequence alignment
Codes for multivariate analysis and long-range statistical correlation
Task 3.
Agents for Data Mining and the Data Model
The kinds of information which need to be gathered and brought to bear on the analysis are diverse, widely distributed, semantically complex, and often interrelated in non-obvious ways.
This part of the proposal will involve developing data-mining resources with intelligent agents to identify and query the appropriate data resources, assemble the information for feedback to pattern recognition and gene modeling programs, and summarize the biological function of the sequence of interest into a report for biologists.
A brief, hypothetical example, can give an indication of the complexity and richness of the analysis paths which may be followed when retrieving the data relevant to describing a DNA sequence.
The protein predicted to be encoded by a newly discovered human gene may have, as its closest known relative, a protein from the nematode C. elegans, and though no function has been discovered for this gene in the worm is part of a gene family with a well characterized member in yeast and another member from E. coli whose 3D structure has been solved by X-ray crystallography.
No less than five separate databases would have to be queried to assemble this information and several of these queries would have been dependent on the success of a previous query.
Often, as in the above example, the best way to present the data may be an image, for instance a visualization of the 3D structure of a protein.
Developing multi-media presentations of the data will be critical which underlines the importance of visualization in Task 5.
Input from a number of experts will be critical to designing and implementing the data mining and inference portion of the analysis engine.
The preceding tasks detailed the general structure of a distributed high performance analysis engine that uses various methods to perform sequence analysis in order to automatically annotate large amounts of sequence data.
The algorithms for sequence analysis can be grouped in roughly two categories: (a) comparison with existing sequences, e.g., similarity based algorithms, and (b) analysis of intrinsic features, e.g., statistical sequence measurements, gene model predictions.
Both categories rely on the accumulated knowledge of existing sequence information, the former for direct comparisons, the latter as the basis for testing new hypotheses and for defining new algorithms.
Building and maintenance of such a knowledge base is crucial to the success of this proposal to provide timely automatic annotation.
It should be emphasized that we do not propose to build an all encompassing database, the mother-of-all-genome-databases, but rather collect all the bits of information that can be used in annotating a sequence and provide reference pointers to more detailed information in the various public genome databases.
Part of the necessary information is already captured in a number of genome databases and we rely on our collaborators at GSDB (NCGR), GDB (JHU), and the biology community to continue the compilation of high quality genome information.
The recent database issue in Nucleic Acid Research (January 1996) provides a good overview of the current status and number of public molecular biology data resources.
We will work with the dozen or so large scale genome sequencing centers on a mutually agreeable data exchange mechanism to import new sequences on a regular basis.
Information that is made public on an ftp server or via the World Wide Web will be incorporated into the data warehouse but treated as a personal communication and not redistributed without further approval by the owner of the information.
The exact policy will have to be defined by the Scientific Advisory Council and likely be modeled after similar services, e.g., EST database searches.
Building and maintaining such a complex dataset can become quite time and resource consuming.
We propose to develop a set of intelligent agents that would assist in this task.
These agents will contact database servers, ftp sites, web pages, etc., to extract the necessary information, and populate the data warehouse.
The agents will need to select and retrieve only relevant data, allowing for fault tolerance for temporary network or database dropouts.
The agents will need to be intelligent enough to handle variation in the underlying data structures, use inferencing to combine facts into higher-level knowledge and affect overall process flow decision-making.
Data Mining Agents
We envision building a set of modular agents to collect information from the various data sources.
An overview of the architecture is given in Figure 7.
[gen-7.gif] Various data resources, e.g., molecular biology databases, ftp sites, web servers, will be accessed by the Data Mining Agents using either database specific tools, e.g., OPM, generic interrogation systems, e.g., Kleisli, or custom software programs to retrieve relevant information and store it in the data warehouse.
The data warehouse keeps track of a minimal amount of information to identify a sequence and link it to relevant information in public databases necessary to perform annotation analysis.
By storing these links in a local data warehouse we can alleviate potential problems associated with timely collection of information from distributed sources over the net.
It is equally important that the high-performance analysis not be hampered by slow or difficult access to a number of databases in order to complete the annotation process.
In order to integrate agents accessing a wide variety of databases and data sources, we plan to develop these agents using the CORBA framework (see CORBA reference).
The CORBA standard was first published by the Object Management Group (OMG) in 1991 and addresses the need for a standardized framework for application integration and interaction across diverse architectures and environments.
OMG, with over 600 members, is the worlds largest software consortium dedicated to fostering interoperability and portability of application software.
The technology has been implemented by about two dozen software companies, many of which are working on their third release of a CORBA compliant product, an indication of the maturity of the technology.
A key feature of the CORBA model is the isolation of the client and server by a well-defined interface that allows the client application to invoke functionality on the server without knowing implementation details.
This affords the software developer total freedom in the way the service is implemented, e.g., the programming language, underlying hardware.
Communication between the client and server is enabled by an Object Request Broker (ORB) that acts as a software bus.
The encapsulating interface is defined using IDL (Interface Definition Language) which can be compiled to generate client stubs and server skeletons in a number of programming languages, e.g., C, C++, Smalltalk, Ada, and more recently Java.
The standard defines both a Static Invocation Interface (SII) for predefined communication interfaces and a Dynamic Invocation Interface (DII) that allows the client to learn about available functions on a particular server.
The DII would allow the agent to dynamically adjust to changing data structures on the server.
At least two companies are working to improve performance over high speed ATM networks, an important feature in the context of this proposal.
The OMG also defined standards for common object services, CORBAservices, that provide standard functionality for 16 application areas, ranging from providing persistent storage, externalizing data, querying databases, to copying, moving, naming services, to more sophisticated trading and transaction services.
Implementations of the CORBA architecture are available from a number of vendors and for a broad range of platforms and operating systems.
Developing the agents using the CORBA architecture will allow us to integrate a variety of different approaches and to utilize existing software tools to access the various data sources, most notably OPM and Kleisli.
These agents would either be launched by the warehouse administrator, e.g., to download new releases, or roam the net, e.g., to launch on-line queries, to collate data for the analysis.
Direct access to the data warehouse circumventing the CORBA Object Request Broker can be implemented if necessary in cases where large amounts of data need to be transferred, e.g., downloading the sequence data.
In these cases, the agent would open a separate channel to pipe data from the source directly to the warehouse.
Since Java will be used for developing user interface tools, whatever architecture is chosen must support Java on the client side.
A number of vendors have announced Java bindings to CORBA and M. Zorn, one of the P.I.s, was able to test interoperability of the BlackWidow/Java implementation with others, e.g., Orbeline/C++ and HP Distributed Smalltalk.
Thus, collaboratory participants could adopt Java as the standard API environment for exchanging data.
Data accessors and translators
The overriding design requirement is to perform any translations from a site-specific data format only once, either at that site or at a middle-tier server site.
The translation process itself depends upon the mechanisms chosen for data exchange and the format of the provided data and will range from relatively trivial to very tedious.
For example, data already available in a DBMS server supporting SQL, e.g., built using OPM, or an object-oriented query language may be retrieved rather easily using OPM.
Filters and/or parsers will be necessary for translating data into a common format.
Through using OPM for the design of the database, many of the translators needed for central databases such as GDB and GSDB can be built automatically.
However, this does not hold true for a number of other databases which have been built using other methods.
For example, translators for OMIM (human disease genes) and metabolic database will have to be built from scratch.
Two additional systems for querying multiple remote sites and producing data translations (Kleisli and MAGPIE, see Appendix II) use different methods and have different strengths.
Most notable in the Kleisli system is the inherent ability to deal with non-relational targets.
The public genome databases GDB and PDB have been implemented using the Object-Protocol Model (OPM) data management tools.
These tools provide facilities for efficiently constructing, maintaining, and exploring Molecular Biology Databases (MBD), using application-specific constructs on top of commercial database management systems (DBMSs).
The OPM tools also provide facilities for reorganizing MBDs and for exploring seamless multiple heterogenous MBDs.
The OPM data management tools have been highly successful in developing new genomic databases, such as GDB 6.0 (released in January 1996) and the relational version of PDB, and in constructing OPM views and interfaces for existing genomic databases such as GSDB.
The OPM data management tools are currently used by over ten groups in USA and Europe.
OPM and the OPM tools are currently being adapted to work in the CORBA framework, i.e., providing a translation from an OPM schema to an IDL definition and developing tools to implement Persistence services on top of OPM style databases.
The OPM tools can be used to design a data model which describes biological objects which are consistent with representations in central databases such as GDB and PDB, which have used the OPM model.
Most importantly, OPM has tools (including translators) for both generating and querying databases, as well as for defining OPM views on existing databases.
This method has been used previously for constructing views of central databases such as GSDB, Genbank (ASN.1), and one could use these tools for bringing all the relevant data from remote databases into a common format.
We may need to extend OPM and the tools according to emerging requirements, however the existing tools provide a good starting point and year one activities would involve creating the metadata infrastructure required by the agents for data mining.
The LBNL OPM team has already done work on how metadata would be organized for multiple genome databases and are quite advanced on developing translators for accessing/querying multiple heterogeneous genome databases.
The CPL/Kleisli system addresses the problem of integrating multiple, distributed heterogeneous data sources and application programs.
One of the strengths of Kleisli is its ability to deal with non-traditional sources, such as data exchange formats, rather than mainstream databases, with the data exchange formats commonly employ complex, nested data structures composed of collection types such as sets, multi-sets, variants, records, lists, and arrays.
Furthermore, relational query languages are notably deficient in their ability to query lists and sequences, precisely the data structures used to represent biosequence information.
Kleisli uses general-purpose query system, CPL/Kleisli, that provides access to a variety of flat files (GenBank), custom systems (ACeDB, ASN.1), application programs (BLAST), and relational databases (GDB, GSDB).
It features a uniform query interface across heterogeneous data sources, a modular and extensible architecture, and most significantly for dealing with the Internet environment, a programmable optimizer.
Kleisli is capable of complex data manipulation such as structural mediation - a complex data \Q\Qjoin'' between structures that come from different sources - and structural wrapping type transformations involving nesting/unnesting plus generalized selections and projections.
Kleisli has been shown to be efficient in composing and executing queries that were considered difficult, if not unanswerable, without first either building a monolithic database or writing highly application-specific integration code.
Detail on the system architecture can be found in Appendix II.
The system is organized into three layers: (1) a CPL query interpreter; (2) an optimizer and application programming interface; (3) Data drivers, modular interfaces that mediate between Kleisli and external data sources.
The basic query interface to Kleisli is the CPL query language (Collection Programming Language) developed at the University of Pennsylvania by Chris Overton, one of our collaborators.
CPL uses the native operations associated with records, variants, sets, lists and multi-sets as the basis of a complete query language for complex types.
CPL currently uses a comprehension syntax, and can be thought of as SQL extended to a richer type system.
Within the context of the Kleisli system, it should be thought of as relatively low-level glue language on top of which user views of an integrated data system can be developed.
The types of data sources currently compatible with Kleisli include Sybase, ACeDB, ASN.1 as well as BLAST; the drivers are generic rather than source specific, meaning that once a Sybase driver has been written any Sybase database can be supported.
Because communication with the drivers is facilitated through UNIX pipes, drivers can be written in any language; we have used C, perl, as well as Prolog.
In addition, a flexible printing routine allows data to be converted to a variety of formats for use in displaying (e.g., HTML) or reading into another programming language (e.g., perl).
We expect that integrating Kleisli into a CORBA-based agent will be straightforward.
MAGPIE (see Appendix II), is an automated system for carrying out genomic sequence analysis.
MAGPIE (Multipurpose Automated Genome Project Investigation Environment) is designed and implemented to meet the challenges that arise with the generation of complete microbial genome sequences, during and beyond the lifetime of a genome sequencing project.
In many ways MAGPIE is a model for methods of analysis and data mining which could become part of the much larger scale processing proposed in this Grand Challenge project.
The Data Model
The ways in which users from the biological community access and interact with the data generated by this project will be a major determinant of the success of the project.
Behind the analysis and interfaces which will be built as a part of this project, it is important to have a rich and robust data model.
The major data being analyzed in this project, nucleotide sequence, can be viewed as the finest level of resolution of a genetic map.
In genomics, maps both genetic and physical, are relatively simple, one-dimensional objects, yet they can communicate a wealth of biological information and can serve as a framework for links to more complex types of information.
Descriptions of chromosomes can be viewed as a series of hierarchical maps proceeding from the banding pattern seen in the light microscope to the actual DNA sequence.
Within the genome community a number of data management structures have been created which capture the essential richness of genetic maps and can serve as a model for organizing and presenting the type of information which will be generated by this project.
It should also be clear from the previous discussion that the annotation process is rather fluid, that is not all users want the same annotation and, because our knowledge of biology is incomplete, we cannot assume that all of the features which would be useful to annotate have even been discovered.
In fact it seems a virtual certainty that entirely new classes of features will be discovered and become part of our analysis within the time-frame of this proposal.
It is important that the data model is sufficiently robust to accommodate the plastic nature of the annotation process.
Information generated in this project will also migrate to a number of public databases which will necessitate the development of export tools and translators, and a compatibility with these community resources is essential.
Since there is great variation in the way data are stored and organized at the various genome centers, it is desirable to establish as much commonality as possible in representing and accessing similar data types when and where possible and feasible.
Thus, considerable effort will be put toward establishing a data model which standardizes common information provided by the various centers while retaining the ability to represent data specific to a site.
The starting points for this effort are the models for existing molecular databases, such as the Genome Database, the Genome Sequence Database, and the Protein Databank.
Beyond the types of data models in the community databases, this project will analyze new types of data and therefore poses new challenges.
For example since we will examine unfinished sequences (partially sequenced clones, sets of contigs with order and gaps, unordered contig sets, partial and gapped gene models), implementation of several new concepts related to how sequence needs to be represented will be required.
These developments may impact the community database schema as well.
A preliminary data model with explicit focus on capturing links to other databases is shown in Figure 8.
[gen-8.gif] The central part of the data model is the object Sequence which characterizes a particular sequence, usually a name and the nucleotide or amino acid sequence.
Relationships among sequences will be captured through subclasses of Relationships, e.g., to establish a relationship between a nucleotide sequence and it's amino acid translation or 3D structure.
Annotation that can be extracted from public databases and those generated by the analysis routines will be handled by the Feature object that references a set of sequence objects.
Each Feature identifies the span to which the annotation applies and has a Link to an existing data resource, i.e. only pointers or accession numbers will be stored in the warehouse.
We do not want to duplicate existing efforts but rather integrate existing knowledge by building a common sequence based index.
The sequence will be used as the key to access all references.
Instead of following hierarchies of relationships, e.g., a gene is similar to a known protein that occurs in a transcription factor database and is referenced by GDB to point to a human disease tabulated in OMIM, we attempt to construct a flat reference profile: once the gene is identified all connections will be available to be included in a report or displayed on a visualization system.
The goal is to provide as much information up front in order to facilitate the interpretation of the results.
The details of the links can be found by accessing the data resources that originated the annotation..
Extracted Information
At a very basic level, the primary question that can be asked of DNA sequence is what biologically relevant features can be associated with the DNA sequence and what can be inferred from these features? What features can and should be located and annotated for newly generated DNA sequence is presently a matter of discussion within the human genome community.
During the spring of 1996 the National Center for Human Genome Research (NCHGR) at the NIH conducted an electronic workshop on the annotation of human DNA sequence to ask ``What annotation should be required before submission of the sequence to a central database?'' Examining the responses of the participants of this workshop can help to identify trends in the thinking of members of the genome community on the subject of sequence annotation.
The first trend that emerges is that there is no consensus on what constitutes appropriate initial annotation of DNA sequence.
The range of opinion runs from ``no annotation is needed'', based on the observation that the sequence itself can be annotated later by individuals with different goals, to ``as much as possible'' (actually a list of more than 50 features or associated data).
Also, the overall strategy for completing the DNA sequence of the human genome is still being worked out (see for example the commentary by Venter et al. (1996), and therefore the structure of the resulting data is still in flux.
There was consensus that certain metadata (discussed below) such as the origin of the clone which was sequenced, needs to be included and this has been taken into account in our data capture plan described previously.
There was also general agreement that, what ever level of annotation is associated with the DNA sequence, the generation of the annotation cannot be allowed to become a bottleneck which slows down the release of the sequence data from the sequencing centers to the user community.
This proposal is directed at alleviating this potential bottleneck.
There are several types of data associated with DNA sequencing projects.
The primary data in a DNA sequencing project are traces of the intensity of florescence, each peak representing a base, collected from automatic DNA sequencing machines.
However, this is almost never the data initially seen by the user community.
Rather the DNA sequence, which is the assembled, interpreted output of a number of sequencing ``runs'', is the data which is presented to the world.
At this stage the data consists of a string made up of four characters ``A'', ``C'', ``G'' and ``T'', generally representing the entire DNA sequence of a clone, which in current practice might contain 30,000 to 150,000 bases.
Generally there are various metadata connected with the sequence at this stage.
These may include the chromosomal origin of the DNA which has been sequenced, where and when the sequencing was preformed, some estimate of the quality of the data, the relationship of the clone sequenced to other clones from the same chromosomal neighborhood, etc.
There is currently no community standard for the kinds of metadata which should be connected to DNA sequence data, but we can anticipate that such standards will emerge over the next year.
The number of fields needed to contain these data will probably be on the order of 25.
The next category of data which will be connected to the DNA sequence will be the identities and locations of features of potential biological importance contained in the sequence.
The annotation of these features is dependent on analysis of the sequence using a number of different methods.
The types of analysis for new DNA sequence regions described in the previous sections will provide a wealth of information and annotation for these sequences.
It this section we consider what is conceptually the next phase of the analysis, integrating the information generated with related information at remote sites, and drawing meaningful biological inferences from all of this.
Underlying this is the need for a comprehensive data model to represent this information and translation methods which bring remote data into a form compatible with this model.
These processes (fusion of relevant information and design of the data model) may present some of the greatest conceptual challenges of this proposal.
Many types of information with potential value for interpreting each newly sequenced genome region must be retrieved from more than a dozen remote databases and remote analysis tools.
Automated methods to retrieve, collate and fuse database information will be implemented to provide insight into a gene's biochemical function, pattern of expression, and organism function, and the corresponding protein's potential structure or structure class, functional family, functional motifs, metabolic role, and potential relationship to disease phenotypes.
Remote analysis systems for many types of DNA and protein analysis are becoming available.
For example, due to new algorithm developments, it may soon be practical to put protein threading systems on-line (see Xu and Uberbacher reprint in Appendix III).
A toolkit of intelligent agents for fast data search and retrieval will need to be developed for the disparate database systems in which this data is stored.
These tools will need to be modular, to provide for the greatest reusability between databases, and be very robust in the face of the many types of data errors which occur in these databases.
The agents will need to select and retrieve only relevant data, allowing for fault tolerance for temporary network or database dropouts.
The agents also will need to be intelligent to handle variation in the underlying data structures, use inferencing to combine facts into higher-level knowledge and affect overall process flow decision-making.
We expect to implement several methods for sequence data capture and input.
(a) The analysis engine will be used for finished sequence clones which have been submitted to central sequence databases such as GSDB (curated by collaboratory participants Fields, Keen and Schad - NCGR).
The basic procedure for this is described in more detail in Appendix II - Procedure for Sequence Analysis from GSDB.
Both automated and interactive procedures for interoperation with the GSDB ``Annotator'' interface tool (Keen et al. 1996 / see Appendix II - NCGR, GSDB and the GSDB Annotator) will be facilitated to allow individual users or the GSDB staff to access the analysis engine from within a GSDB session or during the data submission process.
(b) We will also provide a similar access capability for individual sequencing centers to analyze their finished or partially assembled daily sequence data.
(c) Sequence ``crawler'' processes will poll about a dozen world-wide sites each day and retrieve new sequence data and metadata about the sequenced clones.
Clone metadata will include identifiers assigned by the GDB central mapping database (The Genome Database, JHU - a collaboratory participant) (Fasman et al. 1996), and additional data about the state of the sequence assembly (known sequence gaps, known fragment order or partial order), information about data quality and redundancy, and information related to the underlying physical map will be input.
Standards for the structure of this information will be established with the genome centers and with the advice of the database design teams for GDB (Fasman, Cottingham, Kingsbury) and GSDB (Fields, Keen, Schad) who are collaborators in this project, and where detailed analysis results will eventually be archived.
One of our data mining goals is not to input data from external sources, but to extract high-level summary information and to create and maintain active links to the underlying data at the remote resources.
We will use a hypertexting mechanism to establish these links within the analysis results document and access the Web interface to the remote data sources to utilize the links.
Data Distribution
The sequence data and its annotations (analysis results) will be posted at the project's web site, where it will be publicly available via HTTP connection, either as static ``pages'' or by invoking common gateway interface (CGI) processes.
Procedures for returning information to each respective genome center will also be facilitated using the web and HTTP exchanges as well.
We will participate in arrangements between sequence generators and the GSDB and GDB databases, to input analysis generated in this project into these databases with their sequence submissions.
SubmitData (see Appendix II) can be used to forward these data to the public databases.
Goals for Task 3 include:
year 1
Determine a minimum data model necessary for the analysis engine
Evaluate CORBA, OPM, Kleisli, and MAGPIE systems
Define annotation model
Construct retrieval agents for minimal data model
year 2
Establish full data model for the analysis engine
Construct agents for return of annotation data
Complete translation agents for GDB, GSDB, and other genome databases
Link to protein threading systems
Begin metadata dictionary for hyperlinks to integrate data used in the analysis engine
year 3
Complete metadata dictionary for hyperlinkages to integrate data used in the analysis engine
Implement annotator
Develop syntenic links to multiple organism data resources
Task 4.
Data Warehouse
The data retrieved by the intelligent agents from the distributed database repositories or calculated from within the analysis engine must be efficiently stored in a local repository for optimal access by the analysis engine.
The data warehouse will need to satisfy four criteria, efficient storage of data, high speed communication to the analysis engine, full access by agents for data exchange with external databases, and extensibility for the future data needs (including multimedia objects) for this project.
While the data needs of this project are paramount, consideration must be given to application of this technology to other grand challenges.
Database Architecture
The data warehouse can be seen as a hybrid between completely federated databases and a central data warehouse.
While we do attempt to collect enough information to build sequence based feature indices, we rely on the federation for the detailed thorough information.
This data warehouse is potentially a huge database and therefore needs to be developed based on the data model, consistent with the data translators related to external data sources and with the interface.
The OPM tools mentioned above could be used here as well in collaboration with the GDB and GSDB community database design teams which are in fact participating in the project.
Database technologies which will be evaluated for implementation of the data warehouse on high performance storage systems are DB2 from IBM and the public domain POSTGRES95 database system (see POSTGRES95 reference).
Two ongoing efforts will be leveraged by this project.
The DB2-PE implementation in the HPSS project and the port of POSTGRESS95 to UniTree (M. Gleicher, pers. comm.) which is available at CCS and NERSC.
DB2 is a relational database management system (RDBMS) that enables users to create, update, and control relational databases using Structured Query Language (SQL).
Designed to meet the information needs of small and large businesses alike, it is available on a variety of platforms, including large systems such as MVS/ESA, VM, and VSE; mid-sized systems such as OS/400, AIX, and other UNIX-based systems; and single or LAN-based systems such as OS/2 and Windows.
Data managed by DB2 database servers can be accessed and manipulated by applications on PC workstations running popular operating systems such as OS/2, DOS, and Windows and by applications developed for UNIX workstations from IBM, HP, and SUN.
Support for additional client and server platforms will be added in the future.
DB2 is an open system.
In addition to client platforms provided by IBM, all DB2 database servers allow for open access from any product that supports the Distributed Relational Database Architecture (DRDA) protocol.
This support eliminates the need for expensive add-on components and gateways.
IBM also offers a facility to access any other RDBMS that implement DRDA application server specification.
This support is offered by a companion product called Distributed Database Connection Services (DDCS).
In addition to its data management functions, DB2 includes tools that let users create customized applications for accessing and working with data.
Support is included for the development of multimedia and object-oriented applications.
Development leading to eventual implementation of the POSTGRES DBMS began in 1975.
POSTGRES was designed to handle large scientific datasets containing diverse multimedia data objects (Stonebreaker, 1986).
POSTGRES has been used to implement many different research and production applications, including a medical informatics database, several geographic information systems, and as the primary data manager for the Sequoia 2000 (see Sequoia 2000 reference) scientific computing project.
Illustra Information Technologies, Inc.(a wholly owned subsidiary of Informix Software, Inc.) has picked up the prototype code and commercialized it, providing a maturation path for industry for the lessons learned with this testbed.
POSTGRES95 has the following features:
* Relational.
One of original research goals of the POSTGRES project was to show that an essentially relational DBMS can be extended to handle complex objects, rules, and be highly extensible.
Postgres95 includes features found in most full fledged relational DBMS's such as declarative queries in SQL, query optimization, concurrency control, transactions, and multi-user support.
* Highly Extensible.
POSTGRES95 allows user-defined operators, types, functions, and access methods.
* Object-relational.
Some have used the term ``object-relational'' to describe POSTGRES95 because it supports some object-oriented features such as inheritance.
* Wide user base.
Having been used for quite some time within the scientific community, this DBMS has demonstrated its portability and extensibility.
Candidate database software systems will be evaluated for implementation as the data warehouse on the terabyte storage facilities at the CCS and NERSC.
The data model described in Task 3 - Agents for Data Mining and the Data Model, will allow retention of the location of the distributed source of the data, the results of analysis, and pointers to locations within external data sources relevant to the analyzed sequence.
Some of these data sources include multimedia (gene expression and anatomical objects).
The data warehouse will need to optimize the access times by both the analysis engine itself and by mining agents seeking specific calculated results.
High Performance Storage Systems
The integration of database and high performance storage system technologies will rely on knowledge gained within the High Performance Storage System (HPSS) project.
The High Performance Storage System (see Appendix II) is a software for high rate access and management of digital data within very large storage environments.
HPSS is intended to address the needs of very large high performance computing and data management environments.
HPSS will be of interest in situations having present or future scalability requirements that are very demanding in terms of total storage capacity, data rates, number of objects stored, and number of users.
HPSS is a cooperative development project, originated by IBM government systems and four Department of Energy laboratories, LLNL, LANL, SNL, and ORNL.
Cornell University under NASA Lewis Research Center sponsorship, and NASA Langley Research Center have also contributed to the development of HPSS.
A central technical goal of HPSS is to move large data files between storage devices and parallel or clustered computers at speeds many times faster than today's commercial storage systems software products, and to do this in a way that is more reliable and manageable than is possible with current systems.
In order to accomplish this goal, HPSS employs the following concepts:
* Network-centered architecture * A design based on standard components * Parallel operations * Multiple hierarchies and classes of service * Centrally managed storage * Growth potential
The data warehouse consists of a significant challenge in the marriage of high performance storage systems and database technologies.
Possible architectures will be evaluated based upon efficiency, extensibility, and portability.
The following summarizes our goals for Task 4:
year 1
Evaluate DBMS for data warehouse at the terabyte storage facilities at CCS and NERSC
Implement testbed data warehouse for input data to analysis engine
Integrate CUMULVS data retrieval and storage for analysis engine
Begin implementation of data warehouse
year 2
Full implementation of data warehouse
Integrate data mining agents for automated retrieval to data warehouse
Streamline data I/O processes to high performance storage system
year 3
Fully implement data warehouse distributed among storage facilities
Implement full data model
Extend data warehouse for multi-media data
Create toolkit for data export to genome community databases
Create toolkit for seamless exploration of data warehouse and genome community databases
Task 5.
Visual and Collaboratory Tools
The volume and complexity of the analyzed information and results generated will require an extremely sophisticated approach to data access and visualization.
Users need to interface to the raw data, to the analysis process, to the resulting synthesis of gene models, features, patterns, map data and other information, and to the work of collaborators.
A visual user interface will be developed within the top layer of the PSE which provides access to the developing analysis engine of this grand challenge.
As described in Task 1 - Seamless High Performance Computing in a Problem Solving Environment, the PSE will contain web-based editors, debuggers, profilers, and performance monitors for code development.
In addition it will incorporate emerging conferencing and collaboration web technologies, leveraging technology development with the CME.
All of these tools will be immediately application to other grand challenge programs.
More specific to this GC will be the visualizations of the data within the analysis engine, and the creation of toolkits for user created visual representations.
Code Visualization Toolkit
As described in Task 1, a visual representation of the collaboratively developed analysis engine code will be created in Java.
This user front end will provide a high-level visual code representation, following the model of AT
These Java embedded images will provide for several levels of detail, from the high level view of the entire analysis engine suite, to a single component of the engine, to individual routines.
The visual representations of the parallelization of components of the analysis engine, and the profiling on the high performance computing system will be leveraged from the XPVM and HeNCE packages.
JAVA Data Visualization Toolkit
The analysis running on the high performance computing centers will need to be accessible to the researcher.
A visualization of the status of the analysis must be able to attach and detach at will, must be accessible for any collaborator regardless of location, and must be shared among collaborators.
Extensions of the CUMULVS system for visualization through a message passing paradigm will allow this collaborative monitoring, as well as steering of the simulation to allow the human in the loop.
In conjunction with the raw data the user must be able to quickly understand the status of the progress in the analysis through time-stamping of the data.
Implementation of this visual data representation will be leveraged from the Code Visualization Toolkit described above.
The same high level image display from within JAVA will be applied to data being generated from the analysis engine, with the scaled views representing chromosome, gene and sequence representations.
This will require the integration of CUMULVS data collection, rather than the earlier DSC information being rendered for code.
Modularity of the design will allow this immediate reuse.
As the user traverses the data at the differing scales, the necessary informational links will be dynamically computed by the metadata within the data warehouse.
Connecting the data extraction and mining agents in real time to a visualization through the web will allow the most facile access.
Java browsers will be synchronized for multiple users so distributed researchers can share the same view, and to provide for server-side pushing of the data stream between browsers for real-time data updates.
VRML Data Visualization Toolkit
2D data representations are not sufficient for the complexity of all the data, for example, displaying protein structure.
Within this GC project, the same data exchange used by the 2D java browser will be provided for VRML browsers.
This process will again be achieved through modular components.
The components for data management will be created using a cgi-scripts to negotiate with CUMULVS for data retrieval, and applets for the synchronized views and server-side or inter-browser data push.
Visual Collaboratory User Interface
The final data visualization involves the post-processing situation to allow the scientist to explore the data and metadata generated from the analysis engine, and from the genome community databases.
In addition to the toolkits described above, the wealth of visual displays from the user community will need to be available from the local data warehouse.
Toward this end, collaborative work will be pursued to integrate views such as the Java-based Interactive Genome Browser (Helt and Rubin, Berkeley) with the data warehouse through the agent delivered data objects.
The Java-based Interactive Genome Browser is based on earlier work using CDROM and web dynamic image creation technologies to present FlyBase data.
A prototype browser has been developed using BioTkperl, a high level set of widgets specifically designed to support development of bioinformatics GUI components (Helt, unpublished, Searls, 1995).
[gen-9.gif] This prototype (see Figure 9) displays features along the physical map of Drosophila chromosome 2 from band 34-36, as well as sequence annotations for DS02740, an 83 kb P1 clone located cytologically at 35E6-35F5, and sequenced at the Berkeley Drosophila Genome Project.
The upper part of Figure 9 represents a portion of this Drosophila genome.
The black numbered boxes near the top represent cytogenetic bands, the blue bars at the next level denote regions covered by P1 contigs and the thinner green lines show the position of the individual P1 clones which make up the contigs.
The black lines represent portions of this chromosome that have been sequenced and can be found in the various sequence databases.
Finally the small red boxes represent the position of P-element insertions.
One can increase the level of magnification for this browser.
The lower portion of Figure 9 displays some of the features found in the DNA sequence of one of the P1 clones which spans the junction of chromosomal band 35D and 35E.
In this display the know genes are in black, the maroon boxes represent BLASTX matches to sequences in public databases, green represents genes predicted by the program Genefinder and the purple boxes are protein coding regions predicted by GRAIL.
A number of other types of information can be accessed through this browser.
It is important for visualization tools to allow the user multiple views of the data at various levels of resolution.
When completely converted to the Java environment and by using data communication middleware, this front end browser is likely become an integral part of the visualization solutions available to the genome community.
Other community visual rendering paradigms include, MapViewer a helper application for viewing genomic maps from the Genome Database (GDB, Fasman, et al., 1996) and ACeDB (Durbin and Thierry-Mieg) a data management and visualization environment widely used in the genome community.
[gen-10.gif] Figure 10 shows an ACeDB display of physical and genetic mapping data from mouse chromosome 7 found in a local database maintained at ORNL.
One of the reasons that ACeDB has enjoyed wide popularity in the genome community is that the code is open and freely accessible.
It also makes it easy to incorporate a wide range of multi-media date, in this case an image of the phenotype of mice with a mutation at the P (pink-eyed dilute) locus (the light colored mice), compared to their normal litter-mates.
Other useful interfaces to genomic data include the GSDB Annotator which is an interactive browser and editor client for GSDB (Keen, et al., 1996), and the various forms of Entrez from the National Center for Biotechnology Information (NCBI).
As they evolve, many of these servers will benefit from the Java and Javascript environment for portability and integration into the browser environment.
Forms-based interfaces such as WWW ACE and WWW ENTREZ will also benefit from the data object toolkits described in Task 3, Agents for Data Mining and the Data Model.
The object-oriented methodology provided within the PSE will provide a framework for the development of specialized visual interfaces within the genome community.
A good approach to designing and building these interface tools is to use OPM together with the Genera software developed by one of the project participant's (Stan Letovsky - JHU).
Genera provides a good way of automatically generating form-based Web interfaces.
Letovsky and Markowitz are currently extending OPM with data visualization facilities in another project on collaboratories.
These extensions could be leveraged for the current problem and in collaboration with the GDB interface group.
We believe that a Web interface to the data in preliminary stages of analysis will be useful before results are made available to community databases.
This step-wise process will create a set of libraries to enable genome scientists to create personalized views of the data in this grand challenge without having to be concerned with data retrieval issues.
Each visualization within this project will be integrated within the PSE user interface to provide a coherent environment for this GC containing code development tools, application monitoring, hyperlinked data integration, and collaborative tools.
These modular tools will be portable for distributed visualization within other GC applications.
Web Collaboration and Conferencing Tools
Web-based technologies are developing at an incredible pace.
Technologies such as chat rooms (Chatting) and whiteboards, groupware (HyperNews) and electronic notebooks (see Electronic Notebook Workshop reference), internet phones and whiteboards (CoolTalk), streaming audio (RealAudio), video (ShockWave) will change the way researchers interact.
As these communication technologies mature, they will be incorporated into the primary interface of the PSE to place a wide range of options at the fingertips of researchers.
Concentration on web-based solutions provides for portability to all platforms, and leverages the work of labs, universities and industry.
The following summarizes our goals for Task 5:
year 1
Integrate maturing web-based collaboration tools into primary PSE user interface
Implement information visualizations in JAVA and JavaScript for DSC
Integrate HeNCE and XPVM within the PSE user interface
Evaluate data object exchange format for the genome community to ensure the greatest impact
Begin conversion the JAVA Genome Browser to a modular structure to utilize the data communication agents for remote databases
year 2
Integrate maturing web-based collaboration tools into primary PSE user interface
Adapt JAVA information visualizations for scaled presentations of data orientation and monitoring within the analysis engine
Create cgi interface for VRML visualization of protein structure
Complete conversion the JAVA Genome Browser
year 3
Integrate maturing web-based collaboration tools into primary PSE user interface
Deploy hyperlink information within JAVA data browser to extend to the last step of seamless retrieval of data from genome databases
Complete data translation to VRML, incorporating links defined by the metadata within the data warehouse
..
__________________________________________________________________
Last Modified: 04:42pm PDT, June 21, 1996
