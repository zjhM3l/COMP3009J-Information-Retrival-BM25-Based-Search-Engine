search:
用递归方式读取所有文件夹和子文件夹中的文件。
为制作索引和处理查询添加计时代码以便于测量时间消耗。
在这段代码中，我使用os.walk方法来递归地处理所有文件夹和子文件夹中的文本文件。此外，我还添加了计时代码来测量处理文档和查询所需的时间。

UnicodeDecodeError: 'gbk' codec can't decode byte 0xa0 in position 10: illegal multibyte sequence
这个问题是由于Python尝试使用默认的编码（在Windows中通常是'gbk'）打开文本文件时，遇到无法解码的字节序列导致的。这通常发生在文本文件包含特殊字符或非标准的Unicode字符时。

解决这个问题的一种方法是在打开文件时明确指定编码为'utf-8'，如下所示：
with open(os.path.join(root, filename), 'r', encoding='utf-8') as f:
这样就可以告诉Python使用'utf-8'编码来读取文件，这个编码可以解码更大范围的字符。

如果文件没有.txt扩展名，但它们仍然是文本文件，那么在这种情况下，你可能不能仅仅依靠文件扩展名来判断文件类型。而且，'.DS_Store'文件通常在每个目录中都有，所以你可能需要更精确地指定要跳过的文件。
如果你知道你想要处理的文件的特定命名模式（例如，如果所有的文件名都包含"GX"），你可以修改代码来只处理匹配该模式的文件，像这样：


plus:
现在我们需要构建一个改进版的倒排索引，使用链接列表存储每个词条及其出现的文档ID，并且我们还需要记录每个词在各个文档中的频率。我会尽可能地在不引入外部库的情况下完成这个任务。
以下是一些修改过的代码，我将构建索引的代码放入了process_documents函数中，它现在会返回一个字典，其中键是词，值是另一个字典，内部字典的键是docID，值是该词在该文档中的频率。
然后，我们根据这个新的索引结构对BM25打分函数进行了相应的修改。
这个代码应该能满足你的需求。在这里，我没有使用Python自带的链表数据结构，因为Python的字典在这种情况下是一个非常高效的替代方案。
如果你真的需要使用链表，那么可能需要自己实现，或者使用一些像collections.deque这样的内置数据结构，但是这可能会使代码变得更加复杂，而且对性能的提升可能并不明显。

需要计算所有文档的平均长度，并将其用于BM25的计算中。对于文档频率的计算，我们现在可以直接从倒排索引中获取。